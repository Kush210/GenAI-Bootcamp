{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune Llama3.1 or OpenAI on the following customer support conversation data:\n",
    "# https://huggingface.co/datasets/aibabyshark/insurance_customer_support_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 2.94k/2.94k [00:00<00:00, 38.7kB/s]\n",
      "Downloading data: 100%|██████████| 177k/177k [00:00<00:00, 829kB/s] \n",
      "Generating train split: 100%|██████████| 100/100 [00:00<00:00, 805.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"aibabyshark/insurance_customer_support_conversation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversation', 'topic', 'idx', 'customer_tone', 'agent_tone', 'number_of_exchange', 'agent_name', 'month'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data['train']\n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = data[\"conversation\"]\n",
    "no_of_conversations = data[\"number_of_exchange\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_finetuning_dataset(conversations, file_name):\n",
    "    messages = []\n",
    "    system_prompt = \"You are a customer support agent who specializes in dealing with customer greivances and complaints. You always answer in a polite and professional manner. If you cannot find a solution to the customer's problem, you should escalate the issue to a higher authority.\"\n",
    "    for conversation  in conversations:\n",
    "        text = conversation\n",
    "        text = text.replace(\"**\", \"\")\n",
    "        chunks = text.split(\"\\n\\n\")\n",
    "        if len (chunks) % 2 != 0:\n",
    "            chunks.pop()\n",
    "        for i in range(0,len(chunks),2) :\n",
    "            customer_message = chunks[i].split(\":\")[1]\n",
    "            assistant_message = chunks[i+1].split(\":\")[1]\n",
    "            message = {\"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": f\"{system_prompt}\"},\n",
    "                            {\"role\": \"user\", \"content\": f\"{customer_message}\"},\n",
    "                            {\"role\": \"assistant\", \"content\": f\"{assistant_message}\"},\n",
    "                        ]}\n",
    "            messages.append(message)\n",
    "    \n",
    "    with open(file_name, 'w') as f:\n",
    "        for message in messages:\n",
    "            f.write(f\"{json.dumps(message)}\\n\")\n",
    "    print(f\"Dataset saved to {file_name}\")\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to Train.jsonl\n",
      "Dataset saved to Test.jsonl\n"
     ]
    }
   ],
   "source": [
    "training_file = create_finetuning_dataset(conversations[:75], \"Train.jsonl\")\n",
    "testing_file = create_finetuning_dataset(conversations[75:], \"Test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from tiktoken import get_encoding\n",
    "\n",
    "def validate_and_estimate_finetuning_data(file_path):\n",
    "    # Setup\n",
    "    format_errors = defaultdict(int)\n",
    "    token_counts = []\n",
    "    total_tokens = 0\n",
    "    encoding = get_encoding(\"cl100k_base\")  # For OpenAI models\n",
    "\n",
    "\n",
    "    # Load the dataset\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        dataset = [json.loads(line) for line in f]\n",
    "\n",
    "    for idx, ex in enumerate(dataset):\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "\n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_messages_list\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Validate format\n",
    "        conversation_tokens = 0\n",
    "        assistant_message_found = False\n",
    "\n",
    "        for message in messages:\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "                continue\n",
    "\n",
    "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "            content = message.get(\"content\", None)\n",
    "            function_call = message.get(\"function_call\", None)\n",
    "\n",
    "            if (not content and not function_call) or not isinstance(content, str):\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "\n",
    "            # Count tokens for each message\n",
    "            try:\n",
    "                message_tokens = len(encoding.encode(message.get(\"content\", \"\")))\n",
    "                conversation_tokens += message_tokens\n",
    "            except Exception as e:\n",
    "                format_errors[\"tokenization_error\"] += 1\n",
    "\n",
    "            if message.get(\"role\") == \"assistant\":\n",
    "                assistant_message_found = True\n",
    "\n",
    "        if not assistant_message_found:\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "        token_counts.append(conversation_tokens)\n",
    "        total_tokens += conversation_tokens\n",
    "\n",
    "    # Output results\n",
    "    return {\n",
    "        \"format_errors\": dict(format_errors),\n",
    "        \"token_counts\": token_counts,\n",
    "        \"total_tokens\": total_tokens,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kush_210/Vettura-genai/vettura-genai/Assignments/Capstone__Project_3\n",
      "/home/kush_210/Vettura-genai/vettura-genai/Assignments/Capstone__Project_3/Train.jsonl\n",
      "/home/kush_210/Vettura-genai/vettura-genai/Assignments/Capstone__Project_3/Test.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "training_File_Path = os.path.join(os.getcwd(),\"Train.jsonl\")\n",
    "validation_File_Path = os.path.join(os.getcwd(),\"Test.jsonl\")\n",
    "print(training_File_Path)\n",
    "print(validation_File_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "Format Errors: {}\n",
      "Token Counts per Conversation: [105, 111, 106, 121, 111, 86, 134, 79, 112, 123, 127, 120, 122, 102, 105, 131, 115, 107, 126, 126, 92, 95, 106, 92, 99, 110, 106, 114, 110, 95, 111, 126, 127, 116, 138, 133, 123, 103, 121, 110, 113, 96, 112, 117, 132, 124, 118, 155, 161, 98, 117, 86, 104, 110, 97, 90, 95, 100, 100, 79, 98, 97, 92, 88, 115, 92, 105, 110, 107, 115, 93, 103, 94, 137, 179, 134, 124, 142, 125, 127, 123, 120, 96, 102, 117, 128, 122, 118, 110, 116, 133, 151, 121, 91, 112, 109, 110, 121, 116, 93, 97, 110, 110, 96, 108, 87, 121, 100, 106, 102, 112, 91, 100, 105, 114, 108, 99, 108, 91, 106, 105, 106, 99, 71, 130, 121, 144, 136, 116, 138, 91, 113, 112, 138, 131, 116, 113, 109, 131, 80, 113, 104, 107, 110, 104, 113, 108, 91, 105, 124, 109, 93, 99, 88, 111, 145, 91, 119, 111, 125, 111, 106, 117, 97, 121, 125, 101, 99, 111, 100, 96, 114, 98, 148, 182, 101, 119, 86, 111, 126, 132, 121, 136, 86, 101, 102, 94, 102, 96, 95, 97, 110, 111, 99, 103, 96, 99, 121, 112, 118, 117, 114, 111, 103, 89, 122, 123, 90, 107, 91, 117, 111, 104, 105, 107, 90, 111, 121, 117, 113, 127, 117, 108, 103, 124, 113, 95, 103, 98, 86, 94, 104, 100, 83, 81, 85, 110, 117, 118, 117, 114, 104, 115, 129, 121, 121, 127, 129, 139, 125, 102, 104, 102, 114, 104, 129, 110, 122, 119, 120, 126, 103, 107, 109, 102, 115, 117, 112, 110, 90, 98, 75, 86, 100, 122, 117, 121, 116, 98, 95, 97, 100, 111, 99, 99, 111, 110, 118, 94, 79, 113, 97, 138, 131, 122, 150, 128, 96, 111, 105, 132, 123, 115, 111, 113, 108, 116, 104, 111, 94, 75, 101, 133, 133, 159, 120, 93, 98, 95, 96, 100, 136, 139, 125, 101, 129, 123, 128, 101, 112, 111, 108, 108, 92, 91, 96, 87, 92, 88, 103, 100, 70, 95, 94, 131, 105, 112, 104, 97, 126, 125, 124, 115, 120, 125, 112, 116, 129, 134, 136, 94, 129, 128, 121, 104, 128, 143, 102, 110, 119, 102, 106, 91, 74, 82, 91, 76, 88, 71, 66, 105, 139, 142, 111, 115, 128, 131, 100, 85, 126, 132, 122, 140, 137, 131, 104, 130, 95, 126, 116, 118, 100, 88]\n",
      "Total Tokens: 44567\n",
      "\n",
      "\n",
      "Test Data\n",
      "Format Errors: {}\n",
      "Token Counts per Conversation: [140, 149, 92, 112, 109, 89, 116, 113, 97, 98, 139, 114, 103, 111, 116, 124, 112, 141, 91, 96, 102, 93, 116, 118, 151, 169, 134, 159, 125, 131, 130, 140, 109, 106, 113, 119, 85, 145, 182, 99, 111, 114, 112, 107, 113, 98, 121, 103, 121, 108, 120, 114, 119, 108, 108, 116, 121, 106, 108, 121, 141, 124, 149, 97, 116, 109, 109, 75, 95, 93, 102, 93, 100, 109, 114, 97, 103, 94, 120, 110, 111, 109, 120, 107, 113, 148, 136, 104, 123, 129, 114, 132, 100, 113, 107, 88, 100, 116, 121, 112, 116, 115, 105, 81, 145, 141, 103, 116, 102, 103, 107, 123, 121, 104, 119, 105, 110, 100, 107, 99, 100]\n",
      "Total Tokens: 13812\n"
     ]
    }
   ],
   "source": [
    "## Training data\n",
    "result = validate_and_estimate_finetuning_data(training_File_Path)\n",
    "\n",
    "# Print Results\n",
    "print(\"Training Data\")\n",
    "print(\"Format Errors:\", result[\"format_errors\"])\n",
    "print(\"Token Counts per Conversation:\", result[\"token_counts\"])\n",
    "print(\"Total Tokens:\", result[\"total_tokens\"])\n",
    "\n",
    "result = validate_and_estimate_finetuning_data(validation_File_Path)\n",
    "\n",
    "## Test dataset\n",
    "print(\"\\n\\nTest Data\")\n",
    "print(\"Format Errors:\", result[\"format_errors\"])\n",
    "print(\"Token Counts per Conversation:\", result[\"token_counts\"])\n",
    "print(\"Total Tokens:\", result[\"total_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkush2101999\u001b[0m (\u001b[33mdl_3\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"OpenAI API Key loaded successfully.\")\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing training file: /home/kush_210/Vettura-genai/vettura-genai/Assignments/Capstone__Project_3/Train.jsonl\n",
      "Deleting existing validation file: /home/kush_210/Vettura-genai/vettura-genai/Assignments/Capstone__Project_3/Test.jsonl\n",
      "Training file uploaded: file-G2tHjt5jCfoYCV1QCMEoMm\n",
      "Validation file uploaded: file-Fh16mYLgHKHk85GhasKFbf\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "## create a client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Function to check if a file already exists on OpenAI\n",
    "def get_existing_file_id(filename):\n",
    "    files = client.files.list()\n",
    "    for file in files.data:\n",
    "        if file.filename == filename:\n",
    "            return file.id  # Return the existing file ID\n",
    "    return None  # File does not exist\n",
    "\n",
    "# Function to delete a file by ID\n",
    "def delete_file(file_id):\n",
    "    response = client.files.delete(file_id)\n",
    "    return response.deleted\n",
    "\n",
    "# Check and delete training file\n",
    "file_name = os.path.basename(training_File_Path)\n",
    "training_file_id = get_existing_file_id(file_name)\n",
    "if training_file_id:\n",
    "    print(f\"Deleting existing training file: {training_File_Path}\")\n",
    "    delete_file(training_file_id)\n",
    "\n",
    "# Check and delete validation file\n",
    "file_name = os.path.basename(validation_File_Path)\n",
    "validation_file_id = get_existing_file_id(file_name)\n",
    "if validation_file_id:\n",
    "    print(f\"Deleting existing validation file: {validation_File_Path}\")\n",
    "    delete_file(validation_file_id)\n",
    "\n",
    "# Upload the training file\n",
    "training = client.files.create(\n",
    "    file=open(training_File_Path, \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "print(f\"Training file uploaded: {training.id}\")\n",
    "\n",
    "# Upload the validation file\n",
    "validation = client.files.create(\n",
    "    file=open(validation_File_Path, \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "print(f\"Validation file uploaded: {validation.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FileObject(id='file-Fh16mYLgHKHk85GhasKFbf', bytes=84957, created_at=1740159246, filename='Test.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-G2tHjt5jCfoYCV1QCMEoMm', bytes=272894, created_at=1740159246, filename='Train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-Jfhb1UCJwAu4q8xdJ9uoQT', bytes=1608, created_at=1739733718, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None, expires_at=None), FileObject(id='file-CShKiZYdfURf8GUpAiQinw', bytes=2196, created_at=1739671084, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None, expires_at=None), FileObject(id='file-KHYEvwF6hcaACbiFmShezV', bytes=4503614, created_at=1739669937, filename='cat_dog_test.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-LsXnUL646NieeiPZtMmriQ', bytes=9399300, created_at=1739669935, filename='cat_dog_train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-R7svZ7c3H1eWx6fuBT8fR8', bytes=2176, created_at=1739667655, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None, expires_at=None), FileObject(id='file-D1urbKYTEf1aX7SEzq8y1G', bytes=4503614, created_at=1739666450, filename='cat_dog_test.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-QCaomz7CHyfLdSCBmK65Qv', bytes=9399300, created_at=1739666448, filename='cat_dog_train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-Xorxphm17g4didauDmiwMN', bytes=4503614, created_at=1739666434, filename='cat_dog_test.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-2jixC9sgSG7vkzrmqPhCz7', bytes=9399300, created_at=1739666433, filename='cat_dog_train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-D419p1xu7MYhXTzyYB96BA', bytes=1812, created_at=1739555379, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None, expires_at=None), FileObject(id='file-EZp9pKeEgsiKbohQfcA3cf', bytes=3524, created_at=1739554179, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None, expires_at=None), FileObject(id='file-LTrfat93Mi8SGDMLG2sytj', bytes=4761116, created_at=1739553210, filename='cat_dog_test.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-9azKeu4Bnd1YX66kYGyVGi', bytes=9039476, created_at=1739553208, filename='cat_dog_train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-Ewr26CNmPBXFUnq1YjXsGc', bytes=3540, created_at=1739552066, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None, expires_at=None), FileObject(id='file-VyijQuKASXFGvAmJ5pDcDy', bytes=4751216, created_at=1739551140, filename='cat_dog_test.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-QRWAoDtKYnRdcR2nRbAZu1', bytes=9019676, created_at=1739551138, filename='cat_dog_train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-5hibuw7sBHDAGVGAbpYEdp', bytes=776, created_at=1739550077, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None, expires_at=None)]\n"
     ]
    }
   ],
   "source": [
    "## List all the files to choose its id for fine tuning with it's data\n",
    "files = client.files.list()\n",
    "print(files.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-7NhS02wQSlTX4vZPNsisFmSH', created_at=1740159428, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=4), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zWe7tdzyNIozBnwMRHtVvoQr', result_files=[], seed=940178511, status='validating_files', trained_tokens=None, training_file='file-G2tHjt5jCfoYCV1QCMEoMm', validation_file='file-Fh16mYLgHKHk85GhasKFbf', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='insuarance customer support agent', entity=None, name=None, tags=None, run_id='ftjob-7NhS02wQSlTX4vZPNsisFmSH'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=4)), type='supervised'), user_provided_suffix=None)\n"
     ]
    }
   ],
   "source": [
    "## Paste the file id into the training_file parameter and choose the model and adjust the hyperparameters if you want to tune it\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    training_file= training.id,\n",
    "    validation_file=validation.id,\n",
    "    model = \"gpt-4o-mini-2024-07-18\",\n",
    "    method={\n",
    "        \"type\": \"supervised\",\n",
    "        \"supervised\": {\n",
    "            \"hyperparameters\": {\n",
    "                \"n_epochs\": 4,  # Number of epochs\n",
    "                \"batch_size\": 20,  # Batch size\n",
    "                \"learning_rate_multiplier\": 0.8,  # Learning rate scaling factor\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    integrations= [\n",
    "        {\n",
    "            \"type\": \"wandb\",\n",
    "            \"wandb\": {\n",
    "                \"project\": \"insuarance customer support agent\",\n",
    "                \"tags\": [\"bot\", \"customer support\", \"finetuning\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FineTuningJob(id='ftjob-7NhS02wQSlTX4vZPNsisFmSH', created_at=1740159428, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=4), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zWe7tdzyNIozBnwMRHtVvoQr', result_files=[], seed=940178511, status='validating_files', trained_tokens=None, training_file='file-G2tHjt5jCfoYCV1QCMEoMm', validation_file='file-Fh16mYLgHKHk85GhasKFbf', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='insuarance customer support agent', entity=None, name=None, tags=None, run_id='ftjob-7NhS02wQSlTX4vZPNsisFmSH'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=4)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-tA9IJT8DR3ZlhfsewAVfsUoF', created_at=1739733139, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal::B1eVAPi1', finished_at=1739733714, hyperparameters=Hyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=4), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zWe7tdzyNIozBnwMRHtVvoQr', result_files=['file-Jfhb1UCJwAu4q8xdJ9uoQT'], seed=1718304776, status='succeeded', trained_tokens=97744, training_file='file-NLkGDRER94HaCzCHopZkrk', validation_file='file-4QiEdM282EbZMVphmSXFHD', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='sarcasting_physics_proffessor', entity=None, name=None, tags=None, run_id='ftjob-tA9IJT8DR3ZlhfsewAVfsUoF'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=4)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-d5jI9FM0YuUlqIlrdLZ7XLP7', created_at=1739732769, error=Error(code='invalid_training_file', message='The job failed due to an invalid training file. Invalid file format. Line 1, message 1: Must be a dictionary with a \"role\" key', param='training_file'), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zWe7tdzyNIozBnwMRHtVvoQr', result_files=[], seed=493155071, status='failed', trained_tokens=None, training_file='file-56KnDyKjyHHFHYB6CqgBib', validation_file='file-1R2RekydwxvW1R7rVEmiMo', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='aurobindo_bot_finetuning_project', entity=None, name=None, tags=None, run_id='ftjob-d5jI9FM0YuUlqIlrdLZ7XLP7'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-lCN7qwizkgHyRMVVeKlG5vAh', created_at=1739669940, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-2024-08-06:personal::B1OCwO0o', finished_at=1739671080, hyperparameters=Hyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=5), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-zWe7tdzyNIozBnwMRHtVvoQr', result_files=['file-CShKiZYdfURf8GUpAiQinw'], seed=1468893547, status='succeeded', trained_tokens=341280, training_file='file-LsXnUL646NieeiPZtMmriQ', validation_file='file-KHYEvwF6hcaACbiFmShezV', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='cat_dog_finetuning', entity=None, name=None, tags=None, run_id='ftjob-lCN7qwizkgHyRMVVeKlG5vAh'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=5)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-uoIUXFc8ElVbboTa949kn71H', created_at=1739666458, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-2024-08-06:personal::B1NJcaYI', finished_at=1739667651, hyperparameters=Hyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=5), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-zWe7tdzyNIozBnwMRHtVvoQr', result_files=['file-R7svZ7c3H1eWx6fuBT8fR8'], seed=1909096066, status='succeeded', trained_tokens=341280, training_file='file-QCaomz7CHyfLdSCBmK65Qv', validation_file='file-D1urbKYTEf1aX7SEzq8y1G', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='cat_dog_finetuning', entity=None, name=None, tags=None, run_id='ftjob-uoIUXFc8ElVbboTa949kn71H'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=5)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-RyveaJHsKgnC716SfaGJgUz4', created_at=1739554556, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-2024-08-06:personal::B0u6iJSc', finished_at=1739555375, hyperparameters=Hyperparameters(batch_size=15, learning_rate_multiplier=0.8, n_epochs=3), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-zWe7tdzyNIozBnwMRHtVvoQr', result_files=['file-D419p1xu7MYhXTzyYB96BA'], seed=1703050062, status='succeeded', trained_tokens=209280, training_file='file-9azKeu4Bnd1YX66kYGyVGi', validation_file='file-LTrfat93Mi8SGDMLG2sytj', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='cat_dog_finetuning', entity=None, name=None, tags=None, run_id='ftjob-RyveaJHsKgnC716SfaGJgUz4'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=15, learning_rate_multiplier=0.8, n_epochs=3)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-xI3Hpx12IYH5djQ4gdD36nZG', created_at=1739553256, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-2024-08-06:personal::B0tnNMKa', finished_at=1739554175, hyperparameters=Hyperparameters(batch_size=5, learning_rate_multiplier=0.8, n_epochs=3), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-zWe7tdzyNIozBnwMRHtVvoQr', result_files=['file-EZp9pKeEgsiKbohQfcA3cf'], seed=1504103498, status='succeeded', trained_tokens=209280, training_file='file-9azKeu4Bnd1YX66kYGyVGi', validation_file='file-LTrfat93Mi8SGDMLG2sytj', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='cat_dog_finetuning', entity=None, name=None, tags=None, run_id='ftjob-xI3Hpx12IYH5djQ4gdD36nZG'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=5, learning_rate_multiplier=0.8, n_epochs=3)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-o6QhtgpbeQPJiaa4LHHnhH22', created_at=1739553094, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=5, learning_rate_multiplier=0.8, n_epochs=3), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-zWe7tdzyNIozBnwMRHtVvoQr', result_files=[], seed=560784516, status='cancelled', trained_tokens=None, training_file='file-QRWAoDtKYnRdcR2nRbAZu1', validation_file='file-VyijQuKASXFGvAmJ5pDcDy', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='cat_dog_finetuning', entity=None, name=None, tags=None, run_id='ftjob-o6QhtgpbeQPJiaa4LHHnhH22'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=5, learning_rate_multiplier=0.8, n_epochs=3)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-0GN8xP3ItJCY3cMoukXFtP6b', created_at=1739551141, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-2024-08-06:personal::B0tFINKT', finished_at=1739552062, hyperparameters=Hyperparameters(batch_size=5, learning_rate_multiplier=0.8, n_epochs=3), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-zWe7tdzyNIozBnwMRHtVvoQr', result_files=['file-Ewr26CNmPBXFUnq1YjXsGc'], seed=30068676, status='succeeded', trained_tokens=198162, training_file='file-QRWAoDtKYnRdcR2nRbAZu1', validation_file='file-VyijQuKASXFGvAmJ5pDcDy', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='cat_dog_finetuning', entity=None, name=None, tags=None, run_id='ftjob-0GN8xP3ItJCY3cMoukXFtP6b'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=5, learning_rate_multiplier=0.8, n_epochs=3)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-urOB2Dl85AwTprmyLlLrrEJa', created_at=1739550819, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=5, learning_rate_multiplier=0.8, n_epochs=3), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-zWe7tdzyNIozBnwMRHtVvoQr', result_files=[], seed=266714190, status='cancelled', trained_tokens=None, training_file='file-J52deN4SnAikKVGsgdjS5i', validation_file='file-TkSakvLH23P3CPtWQmpdi2', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='cat_dog_finetuning', entity=None, name=None, tags=None, run_id='ftjob-urOB2Dl85AwTprmyLlLrrEJa'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=5, learning_rate_multiplier=0.8, n_epochs=3)), type='supervised'), user_provided_suffix=None)]\n"
     ]
    }
   ],
   "source": [
    "## Listing all the recent jobs\n",
    "all_jobs = client.fine_tuning.jobs.list(limit=10).data\n",
    "print(all_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-7NhS02wQSlTX4vZPNsisFmSH', created_at=1740159428, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=4), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zWe7tdzyNIozBnwMRHtVvoQr', result_files=[], seed=940178511, status='validating_files', trained_tokens=None, training_file='file-G2tHjt5jCfoYCV1QCMEoMm', validation_file='file-Fh16mYLgHKHk85GhasKFbf', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='insuarance customer support agent', entity=None, name=None, tags=None, run_id='ftjob-7NhS02wQSlTX4vZPNsisFmSH'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=4)), type='supervised'), user_provided_suffix=None)\n",
      "FineTuningJob(id='ftjob-7NhS02wQSlTX4vZPNsisFmSH', created_at=1740159428, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=4), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zWe7tdzyNIozBnwMRHtVvoQr', result_files=[], seed=940178511, status='validating_files', trained_tokens=None, training_file='file-G2tHjt5jCfoYCV1QCMEoMm', validation_file='file-Fh16mYLgHKHk85GhasKFbf', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='insuarance customer support agent', entity=None, name=None, tags=None, run_id='ftjob-7NhS02wQSlTX4vZPNsisFmSH'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=4)), type='supervised'), user_provided_suffix=None)\n"
     ]
    }
   ],
   "source": [
    "## Prinint the recent job to get the fine-tuned model name\n",
    "print(all_jobs[0])\n",
    "print(client.fine_tuning.jobs.retrieve(all_jobs[0].id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: validating_files\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: validating_files\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: validating_files\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: validating_files\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: validating_files\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: validating_files\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: validating_files\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: validating_files\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: validating_files\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: validating_files\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: validating_files\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: validating_files\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: validating_files\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: validating_files\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-7NhS02wQSlTX4vZPNsisFmSH\n",
      "Status: succeeded\n",
      "Fine-tuning job succeeded.\n",
      "Status: FineTuningJob(id='ftjob-7NhS02wQSlTX4vZPNsisFmSH', created_at=1740159428, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal::B3RSLUDL', finished_at=1740160223, hyperparameters=Hyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=4), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zWe7tdzyNIozBnwMRHtVvoQr', result_files=['file-4Zpyu7d2ZdeW3HtCwvp8sE'], seed=940178511, status='succeeded', trained_tokens=195156, training_file='file-G2tHjt5jCfoYCV1QCMEoMm', validation_file='file-Fh16mYLgHKHk85GhasKFbf', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='insuarance customer support agent', entity=None, name=None, tags=None, run_id='ftjob-7NhS02wQSlTX4vZPNsisFmSH'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=20, learning_rate_multiplier=0.8, n_epochs=4)), type='supervised'), user_provided_suffix=None)\n",
      "Model Name: ft:gpt-4o-mini-2024-07-18:personal::B3RSLUDL\n",
      "Result file id: file-4Zpyu7d2ZdeW3HtCwvp8sE\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "checkpoints = None\n",
    "\n",
    "# Function to get the latest accuracy and loss from checkpoints\n",
    "def get_latest_accuracy(job_id, api_key):\n",
    "    url = f\"https://api.openai.com/v1/fine_tuning/jobs/{job_id}/checkpoints\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    checkpoints = response.json().get(\"data\", [])\n",
    "\n",
    "    if not checkpoints:\n",
    "        return None, None  # Return None if no checkpoints are available\n",
    "\n",
    "    # Find the latest checkpoint based on step_number\n",
    "    latest_checkpoint = max(checkpoints, key=lambda c: c[\"step_number\"])\n",
    "    latest_accuracy = latest_checkpoint[\"metrics\"][\"full_valid_mean_token_accuracy\"]\n",
    "    latest_loss = latest_checkpoint[\"metrics\"][\"full_valid_loss\"]\n",
    "    return latest_accuracy, latest_loss\n",
    "\n",
    "# Function to monitor fine-tuning job and print training/validation metrics\n",
    "def monitor_finetuning_progress(job_id, api_key, check_interval=10):\n",
    "    while True:\n",
    "        try:\n",
    "            # Retrieve the fine-tuning job status\n",
    "            job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "            # Print basic job details\n",
    "            print(f\"Job ID: {job_status.id}\")\n",
    "            print(f\"Status: {job_status.status}\")\n",
    "\n",
    "            # Check if the job has completed\n",
    "            if job_status.status in [\"succeeded\", \"failed\"]:\n",
    "                print(f\"Fine-tuning job {job_status.status}.\")\n",
    "                model_id = job_status.fine_tuned_model\n",
    "                result_file_id = job_status.result_files[0]\n",
    "                return job_status, model_id, result_file_id\n",
    "            \n",
    "            # Retrieve and print the latest accuracy and loss\n",
    "            latest_accuracy, latest_loss = get_latest_accuracy(job_id, api_key)\n",
    "            if latest_accuracy is not None and latest_loss is not None:\n",
    "                print(f\"Latest Accuracy: {latest_accuracy:.3f}\")\n",
    "                print(f\"Latest Loss: {latest_loss:.3f}\")\n",
    "            else:\n",
    "                print(\"No checkpoints available yet.\")\n",
    "                \n",
    "            # Wait before the next check\n",
    "            print(f\"Checking again in {check_interval} seconds...\\n\")\n",
    "            time.sleep(check_interval)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}. Retrying in {check_interval} seconds...\\n\")\n",
    "            time.sleep(check_interval)\n",
    "\n",
    "\n",
    "# Replace `fine_tuning_job_id` with your actual job ID\n",
    "fine_tuning_job_id = all_jobs[0].id\n",
    "status, model_name, result_file_id = monitor_finetuning_progress(fine_tuning_job_id, api_key, 10)\n",
    "print(f\"Status: {status}\")\n",
    "print(f\"Model Name: {model_name}\")\n",
    "print(f\"Result file id: {result_file_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object': 'fine_tuning.job.checkpoint', 'id': 'ftckpt_gXpuojhm2fm1SlFh0MBzpWHs', 'created_at': 1740160183, 'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::B3RSLUDL', 'fine_tuning_job_id': 'ftjob-7NhS02wQSlTX4vZPNsisFmSH', 'metrics': {'step': 81}, 'step_number': 81}\n",
      "{'object': 'fine_tuning.job.checkpoint', 'id': 'ftckpt_RpupL3VQ3LLUfucdtMB58teX', 'created_at': 1740160077, 'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::B3RSKWIa:ckpt-step-63', 'fine_tuning_job_id': 'ftjob-7NhS02wQSlTX4vZPNsisFmSH', 'metrics': {'step': 63}, 'step_number': 63}\n",
      "{'object': 'fine_tuning.job.checkpoint', 'id': 'ftckpt_uPrmzWq0wjsmjvr7BqSSfYSN', 'created_at': 1740159961, 'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::B3RSK9Ya:ckpt-step-42', 'fine_tuning_job_id': 'ftjob-7NhS02wQSlTX4vZPNsisFmSH', 'metrics': {'step': 42}, 'step_number': 42}\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\n",
    "    f\"https://api.openai.com/v1/fine_tuning/jobs/{all_jobs[0].id}/checkpoints\",\n",
    "    headers={\"Authorization\": f\"Bearer {api_key}\"}\n",
    ")\n",
    "checkpoints = response.json().get(\"data\", [])\n",
    "for checkpoint in checkpoints:\n",
    "    print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result File Contents:\n",
      "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\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def print_result_file_content(file_id, api_key):\n",
    "    # API endpoint to retrieve file content\n",
    "    url = f\"https://api.openai.com/v1/files/{file_id}/content\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "    # Request the file content\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Print the contents of the file\n",
    "        print(\"Result File Contents:\")\n",
    "        print(response.text)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve file content. Status Code: {response.status_code}\")\n",
    "        print(f\"Error: {response.json()}\")\n",
    "\n",
    "# Print the result file content\n",
    "print_result_file_content(result_file_id, api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "## Inferencing the fine tuned model\n",
    "def query(user_input,model_name):\n",
    "  completion = client.chat.completions.create(\n",
    "      model= model_name,\n",
    "      messages=[\n",
    "          {\"role\": \"user\", \"content\": user_input }\n",
    "      ],\n",
    "      temperature=0.7,\n",
    "  )\n",
    "\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Hi, I have an issue with my insurance policy.\",\n",
    "    \"I need help with a claim I submitted last week.\",\n",
    "    \"I want to cancel my policy and get a refund.\",\n",
    "    \"I need to update my contact information.\",\n",
    "    \"I'm having trouble accessing my account online.\",\n",
    "    \"I need to speak with a customer service representative.\",\n",
    "    \"I want to know the status of my claim.\",\n",
    "    \"I need to file a complaint about my claim.\",\n",
    "    \"I have a question about my coverage options.\",\n",
    "    \"I want to add a new driver to my policy.\",\n",
    "]\n",
    "model = \"ft:gpt-4o-mini-2024-07-18:personal::B3RSLUDL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Hi, I have an issue with my insurance policy.\n",
      "------------------------------------------------------------\n",
      " Response:  I'm here to help. What seems to be the problem with your policy?\n",
      "\n",
      "============================================================\n",
      "Query: I need help with a claim I submitted last week.\n",
      "------------------------------------------------------------\n",
      " Response:  Sure! Could you provide me with your claim number and a brief description of the issue you're experiencing?\n",
      "\n",
      "============================================================\n",
      "Query: I want to cancel my policy and get a refund.\n",
      "------------------------------------------------------------\n",
      " Response:  To assist you better, could you please provide your policy number and the reason for the cancellation?\n",
      "\n",
      "============================================================\n",
      "Query: I need to update my contact information.\n",
      "------------------------------------------------------------\n",
      " Response:  I can't assist with personal data updates. Please follow the provided link or contact customer service directly for help.\n",
      "\n",
      "============================================================\n",
      "Query: I'm having trouble accessing my account online.\n",
      "------------------------------------------------------------\n",
      " Response:  I can help you with that. Could you please specify which account you're trying to access?\n",
      "\n",
      "============================================================\n",
      "Query: I need to speak with a customer service representative.\n",
      "------------------------------------------------------------\n",
      " Response:  I can assist you with your inquiry. What issue are you experiencing?\n",
      "\n",
      "============================================================\n",
      "Query: I want to know the status of my claim.\n",
      "------------------------------------------------------------\n",
      " Response:  To assist you effectively, could you please provide your claim number or any other relevant details?\n",
      "\n",
      "============================================================\n",
      "Query: I need to file a complaint about my claim.\n",
      "------------------------------------------------------------\n",
      " Response:  I'm here to help. Could you provide me with the details of your claim and the specific issue you're experiencing?\n",
      "\n",
      "============================================================\n",
      "Query: I have a question about my coverage options.\n",
      "------------------------------------------------------------\n",
      " Response:  Of course! What specifically would you like to know about your coverage options?\n",
      "\n",
      "============================================================\n",
      "Query: I want to add a new driver to my policy.\n",
      "------------------------------------------------------------\n",
      " Response:  Adding a new driver to your policy is a straightforward process. To start, do you have the new driver's details ready, such as their full name, date of birth, and driver's license number?\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "for q in queries:\n",
    "    print(f\"Query: {q}\")\n",
    "    print(\"---\"*20)\n",
    "    response = query(q, model)\n",
    "    print(f\" Response: {response}\\n\")\n",
    "    print(\"===\"*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vettura",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
