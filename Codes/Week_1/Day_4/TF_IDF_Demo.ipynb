{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZjOT4GGZiTE"
      },
      "source": [
        "**TF-IDF** can be seen as a function that ranks words based on their importance accross documents, weighted down by the amout of times they actually appear, following the idea that if a word is way too common, it shouldn't be that important.\n",
        "\n",
        "Mathematically, it is the equivalent of calculating the _term frequency_ (TF) multiplied by the _inverse document frequency_ (IDF). So, given a term _t_ and a document _d_. For our `TF` we'd have:\n",
        "\n",
        "$$TF(t,d) = \\frac{count(t,d)}{count(*,d)}$$\n",
        "\n",
        "Where `count(t,d)` is the amount of times `t` appears inside of `d`, and `count(*,d)` is the total number of terms inside of `d`, meaning that, if a word appears a lot in our documents, that means that it's probably somewhat revelant.\n",
        "\n",
        "But, what about words that are actually common but don't bring any value for us? That's where IDF comes in, it's a way to weight down such common words by comparing its appearance among all of our documents. So, if a word appears way too much across many documents, it probably means that it isn't that relevant and it's just a common word. Here's what it looks like in maths language:\n",
        "\n",
        "$$IDF(t,D) = log_e(\\frac{D}{amount(t,D)})$$\n",
        "\n",
        "Where `D` is the total number of documents, and `amount(t,D)` is the number of documents containing `t`.\n",
        "The $log_e$ is due to the fact that the `IDF` would explode for the cases where we have too many documents.\n",
        "\n",
        "Finally, we have:\n",
        "\n",
        "$$TF\\text{-}IDF = TF(t,d)*IDF(t,D)$$\n",
        "\n",
        "Here we'll learn how to implement and make proper use of it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"Thor eating pizza, Loki is eating pizza, Ironman ate pizza already\",\n",
        "    \"Apple is announcing new iphone tomorrow\",\n",
        "    \"Tesla is announcing new model-3 tomorrow\",\n",
        "    \"Google is announcing new pixel-6 tomorrow\",\n",
        "    \"Microsoft is announcing new surface tomorrow\",\n",
        "    \"Amazon is announcing new eco-dot tomorrow\",\n",
        "    \"I am eating biryani and you are eating grapes\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#let's create the vectorizer and fit the corpus and transform them accordingly\n",
        "v = TfidfVectorizer()\n",
        "v.fit(corpus)\n",
        "transform_output = v.transform(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'thor': 25, 'eating': 10, 'pizza': 22, 'loki': 17, 'is': 16, 'ironman': 15, 'ate': 7, 'already': 0, 'apple': 5, 'announcing': 4, 'new': 20, 'iphone': 14, 'tomorrow': 26, 'tesla': 24, 'model': 19, 'google': 12, 'pixel': 21, 'microsoft': 18, 'surface': 23, 'amazon': 2, 'eco': 11, 'dot': 9, 'am': 1, 'biryani': 8, 'and': 3, 'you': 27, 'are': 6, 'grapes': 13}\n",
            "28\n"
          ]
        }
      ],
      "source": [
        "#let's print the vocabulary\n",
        "\n",
        "print(v.vocabulary_)\n",
        "print(len(v.vocabulary_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "already : 2.386294361119891\n",
            "am : 2.386294361119891\n",
            "amazon : 2.386294361119891\n",
            "and : 2.386294361119891\n",
            "announcing : 1.2876820724517808\n",
            "apple : 2.386294361119891\n",
            "are : 2.386294361119891\n",
            "ate : 2.386294361119891\n",
            "biryani : 2.386294361119891\n",
            "dot : 2.386294361119891\n",
            "eating : 1.9808292530117262\n",
            "eco : 2.386294361119891\n",
            "google : 2.386294361119891\n",
            "grapes : 2.386294361119891\n",
            "iphone : 2.386294361119891\n",
            "ironman : 2.386294361119891\n",
            "is : 1.1335313926245225\n",
            "loki : 2.386294361119891\n",
            "microsoft : 2.386294361119891\n",
            "model : 2.386294361119891\n",
            "new : 1.2876820724517808\n",
            "pixel : 2.386294361119891\n",
            "pizza : 2.386294361119891\n",
            "surface : 2.386294361119891\n",
            "tesla : 2.386294361119891\n",
            "thor : 2.386294361119891\n",
            "tomorrow : 1.2876820724517808\n",
            "you : 2.386294361119891\n"
          ]
        }
      ],
      "source": [
        "#let's print the idf of each word:\n",
        "\n",
        "all_feature_names = v.get_feature_names_out()\n",
        "\n",
        "for word in all_feature_names:\n",
        "    \n",
        "    #let's get the index in the vocabulary\n",
        "    indx = v.vocabulary_.get(word)\n",
        "    \n",
        "    #get the score\n",
        "    idf_score = v.idf_[indx]\n",
        "    \n",
        "    print(f\"{word} : {idf_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Thor eating pizza, Loki is eating pizza, Ironman ate pizza already', 'Apple is announcing new iphone tomorrow']\n",
            "[0.24266547 0.         0.         0.         0.         0.\n",
            " 0.         0.24266547 0.         0.         0.40286636 0.\n",
            " 0.         0.         0.         0.24266547 0.11527033 0.24266547\n",
            " 0.         0.         0.         0.         0.72799642 0.\n",
            " 0.         0.24266547 0.         0.        ]\n"
          ]
        }
      ],
      "source": [
        "print(corpus[:2])\n",
        "#let's print the transformed output from tf-idf\n",
        "v1 = transform_output.toarray()[0]\n",
        "v2 = transform_output.toarray()[1]\n",
        "v3 = transform_output.toarray()[2]\n",
        "\n",
        "print(v1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this you can see now we can have vector representation of any document. These reprentations can be processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import euclidean, cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Euclidean distance between v1 and v2: 1.392046685146447\n",
            "Euclidean distance between v3 and v2: 1.1360708006098064\n",
            "Cosine similarity (1 - cosine) between v1 and v2: 0.0311030131863943\n",
            "Cosine similarity (1 - cosine) between v3 and v2: 0.35467156800089694\n"
          ]
        }
      ],
      "source": [
        "# Calculating distances\n",
        "distance_v1_v2 = euclidean(v1, v2)\n",
        "distance_v3_v2 = euclidean(v3, v2)\n",
        "\n",
        "cosine_v1_v2 = cosine(v1, v2)\n",
        "cosine_v3_v2 = cosine(v3, v2)\n",
        "\n",
        "# Output results\n",
        "print(\"Euclidean distance between v1 and v2:\", distance_v1_v2)\n",
        "print(\"Euclidean distance between v3 and v2:\", distance_v3_v2)\n",
        "print(\"Cosine similarity (1 - cosine) between v1 and v2:\", 1 - cosine_v1_v2)\n",
        "print(\"Cosine similarity (1 - cosine) between v3 and v2:\", 1 - cosine_v3_v2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
