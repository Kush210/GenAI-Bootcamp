{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QqRIowDeyCw"
      },
      "source": [
        "Note:\n",
        "1. Get key from [here](https://console.groq.com/keys?_gl=1*1ih2ghe*_gcl_au*MTYyNDMzNzQ4OC4xNzM0OTI3MjM3*_ga*MTYzMDI0MzAxOC4xNzI3MDg1NzIx*_ga_4TD0X2GEZG*MTczNTcxMTUxOS42LjEuMTczNTcxMTY0OS42MC4wLjA.)  and store in `secrets` of `colab` with name `GROQ_API_KEY`\n",
        "2. Install groq: `!pip install groq `\n",
        "\n",
        "---\n",
        "Groq emerged as the first API provider to break the 100 tokens per second generation rate while running Meta’s Llama2-70B parameter model.\n",
        "\n",
        "Groq currently hosts a variety of open-source large language models running on its LPUs (Language Processing Unit) for public access. Access to these demos are available through Groq's website.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ref: https://console.groq.com/docs/text-chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF6xqZN4wh0G",
        "outputId": "3227c0eb-525e-423c-ffee-5aa523e1ef93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.13.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
            "Downloading groq-0.13.1-py3-none-any.whl (109 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/109.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHi-Ldh8ygbn"
      },
      "source": [
        "# Text completion API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KkNfSzl9do_9"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Execute the following block only if you are using colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "client = Groq(\n",
        "    api_key=userdata.get(\"GROQ_API_KEY\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### If you are on local machine use this for authentication. Add key to .env file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq client initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables from the .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Retrieve the API key from the environment\n",
        "api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "if api_key:\n",
        "    client = Groq(api_key=api_key)\n",
        "    print(\"Groq client initialized successfully.\")\n",
        "else:\n",
        "    print(\"GROQ_API_KEY not found in .env. Please check your configuration.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rn6s6bD7NgV"
      },
      "source": [
        "Parameters:\n",
        "1. **frequency_penalty**  \n",
        "   - `number or null` (Optional)  \n",
        "   - Defaults to 0  \n",
        "   - Accepts a number between `-2.0 and 2.0`. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same lines.\n",
        "\n",
        "2. **max_tokens**  \n",
        "   - `integer or null` (Optional)  \n",
        "   - The maximum number of tokens that can be generated in the chat completion.  \n",
        "   - The total length of input tokens and generated tokens is limited by the model's context length.\n",
        "\n",
        "3. **messages**  \n",
        "   - `array` (Required)  \n",
        "   - A list of messages comprising the conversation so far.\n",
        "\n",
        "4. **model**  \n",
        "   - `string` (Required)  \n",
        "   - The ID of the model to use.\n",
        "\n",
        "1. **presence_penalty**   `number or null` (Optional)  \n",
        "   - Defaults to 0  \n",
        "   - Number between `-2.0 and 2.0`. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to discuss new topics.\n",
        "\n",
        "2. **response_format**  \n",
        "   - `object or null` (Optional)  \n",
        "   - An object specifying the format in which the model must output.  \n",
        "   - Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees that the message generated by the model is valid JSON.  \n",
        "   - **Important**: When using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message.\n",
        "\n",
        "3. **stream**  \n",
        "   - `boolean or null` (Optional)  \n",
        "   - Defaults to `false`  \n",
        "   - If set to `true`, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.\n",
        "\n",
        "4. **temperature**  \n",
        "   - `number or null` (Optional)  \n",
        "   - Defaults to 1  \n",
        "   - What sampling temperature to use, between 0 and 2. Higher values like `0.8` will make the output more random, while lower values like `0.2` will make it more focused and deterministic. We generally recommend altering this or `top_p`, but not both.\n",
        "\n",
        "5. **tool_choice**  \n",
        "   - `string / object or null` (Optional)  \n",
        "   - Controls which (if any) tool is called by the model.  \n",
        "   - `none` means the model will not call any tool and instead generates a message.  \n",
        "   - `auto` means the model can pick between generating a message or calling one or more tools.  \n",
        "   - `required` means the model must call one or more tools.  \n",
        "   - Specifying a particular tool via `{ \"type\": \"function\", \"function\": { \"name\": \"my_function\" } }` forces the model to call that tool.  \n",
        "   - `none` is the default when no tools are present. `auto` is the default if tools are present.\n",
        "\n",
        "6. **tools**  \n",
        "   - `array or null` (Optional)  \n",
        "   - A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\n",
        "\n",
        "7. **top_logprobs**  \n",
        "   - `integer or null` (Optional)  \n",
        "   - This is not yet supported by any of our models. An integer between `0 and 20` specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.\n",
        "\n",
        "8. **top_p**  \n",
        "   - `number or null` (Optional)  \n",
        "   - Defaults to `1`  \n",
        "   - An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So `0.1` means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature`, but not both.\n",
        "\n",
        "9. **user**  \n",
        "   - `string or null` (Optional)  \n",
        "   - A unique identifier representing your end-user, which can help us monitor and detect abuse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7EMniuKwQ0x",
        "outputId": "e388871d-32f8-415c-c166-060dbd6b4550"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yoga is an ancient Indian practice that combines physical postures, breathing techniques, and meditation to promote physical, mental, and emotional well-being. The importance of yoga can be understood from several perspectives:\n",
            "\n",
            "**Physical Benefits:**\n",
            "\n",
            "1. **Flexibility and Strength**: Yoga helps increase flexibility, balance, and strength by stretching and toning the muscles.\n",
            "2. **Improved Posture**: Regular yoga practice improves posture, reducing the risk of back and neck pain.\n",
            "3. **Weight Management**: Yoga can help with weight loss and maintenance by improving metabolism and reducing stress.\n",
            "4. **Improved Sleep**: Yoga can help regulate sleep patterns, leading to better rest and recovery.\n",
            "\n",
            "**Mental and Emotional Benefits:**\n",
            "\n",
            "1. **Reduced Stress and Anxiety**: Yoga has been shown to reduce stress and anxiety by promoting relaxation and calmness.\n",
            "2. **Improved Mental Clarity**: Yoga improves focus, concentration, and mental clarity, making it easier to tackle daily tasks.\n",
            "3. **Enhanced Self-Awareness**: Yoga helps develop self-awareness, self-acceptance, and self-compassion, leading to a more positive self-image.\n",
            "4. **Emotional Regulation**: Yoga teaches breathing techniques and meditation practices that help regulate emotions, reducing mood swings and emotional reactivity.\n",
            "\n",
            "**Holistic Benefits:**\n",
            "\n",
            "1. **Increased Mind-Body Connection**: Yoga cultivates a deeper connection between the mind, body, and spirit, promoting overall well-being.\n",
            "2. **Improved Immune Function**: Yoga has been shown to boost the immune system, reducing the risk of illnesses and infections.\n",
            "3. **Increased Energy**: Yoga can increase energy levels, reducing fatigue and improving overall vitality.\n",
            "4. **Spiritual Growth**: Yoga provides a framework for spiritual growth, self-reflection, and personal development.\n",
            "\n",
            "**Therapeutic Benefits:**\n",
            "\n",
            "1. **Pain Management**: Yoga can help manage chronic pain, reducing the need for medication and improving quality of life.\n",
            "2. **Injury Recovery**: Yoga can aid in injury recovery by promoting flexibility, strength, and range of motion.\n",
            "3. **Mental Health**: Yoga has been shown to be beneficial for mental health conditions such as depression, anxiety, and post-traumatic stress disorder (PTSD).\n",
            "4. **Chronic Disease Management**: Yoga can help manage chronic diseases such as diabetes, hypertension, and heart disease.\n",
            "\n",
            "In summary, yoga is a holistic practice that offers a wide range of benefits, from physical and mental well-being to spiritual growth and therapeutic applications. By incorporating yoga into your daily routine, you can experience these benefits and improve your overall quality of life.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"you are a helpful assistant.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of Yoga\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmOZFcB2yyI9"
      },
      "source": [
        "# Audio transcription API\n",
        "Transcribes audio into the input language.\n",
        "\n",
        "Parameters:\n",
        "1. **file**  \n",
        "   - `string` (Required)  \n",
        "   - The audio file object (not the file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n",
        "\n",
        "2. **language**  \n",
        "   - `string` (Optional)  \n",
        "   - The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency.\n",
        "\n",
        "3. **model**  \n",
        "   - `string` (Required)  \n",
        "   - ID of the model to use. Only `whisper-large-v3` is currently available.\n",
        "\n",
        "4. **prompt**  \n",
        "   - `string` (Optional)  \n",
        "   - An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.\n",
        "\n",
        "5. **response_format**  \n",
        "   - `string` (Optional)  \n",
        "   - Defaults to `json`  \n",
        "   - The format of the transcript output, in one of these options: `json`, `text`, or `verbose_json`.\n",
        "\n",
        "6. **temperature**  \n",
        "   - `number` (Optional)  \n",
        "   - Defaults to `0`  \n",
        "   - The sampling temperature, between `0` and `1`. Higher values like `0.8` will make the output more random, while lower values like `0.2` will make it more focused and deterministic. If set to `0`, the model will use log probability to automatically adjust the temperature until certain thresholds are met.\n",
        "\n",
        "7. **timestamp_granularities**  \n",
        "   - `array` (Optional)  \n",
        "   - Defaults to `segment`  \n",
        "   - The timestamp granularities to populate for this transcription. `response_format` must be set to `verbose_json` to use timestamp granularities. Either or both of these options are supported: `word`, or `segment`.  \n",
        "   - Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/ashish/Desktop/vettura-genai/Codes\n"
          ]
        }
      ],
      "source": [
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPrKOoTdyxo2",
        "outputId": "bfc5cf02-4b26-4ea9-8e60-f34656a8d3d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The fire that warms us can also consume us. It is not the fault of the fire.\n"
          ]
        }
      ],
      "source": [
        "filename = \"./Week_1/Day_5/transcription.mp3\"\n",
        "\n",
        "with open(filename, \"rb\") as file:\n",
        "    transcription = client.audio.transcriptions.create(\n",
        "      file=(filename, file.read()),\n",
        "      model=\"whisper-large-v3\",\n",
        "      prompt=\"Specify context or spelling\",\n",
        "      response_format=\"json\",\n",
        "      language=\"en\",\n",
        "      temperature=0.0\n",
        "      )\n",
        "    print(transcription.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8QptJIG6HKR"
      },
      "source": [
        "# Audio translation API\n",
        "\n",
        "Translates audio into English.\n",
        "\n",
        "Parameters:\n",
        "1. **file**  \n",
        "   - `string` (Required)  \n",
        "   - The audio file object (not the file name) to translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n",
        "\n",
        "2. **model**  \n",
        "   - `string` (Required)  \n",
        "   - ID of the model to use. Only `whisper-large-v3` is currently available.\n",
        "\n",
        "3. **prompt**  \n",
        "   - `string` (Optional)  \n",
        "   - An optional text to guide the model's style or continue a previous audio segment. The prompt should be in English.\n",
        "\n",
        "4. **response_format**  \n",
        "   - `string` (Optional)  \n",
        "   - Defaults to `json`  \n",
        "   - The format of the translation output, in one of these options: `json`, `text`, or `verbose_json`.\n",
        "\n",
        "5. **temperature**  \n",
        "   - `number` (Optional)  \n",
        "   - Defaults to `0`  \n",
        "   - The sampling temperature, between `0` and `1`. Higher values like `0.8` will make the output more random, while lower values like `0.2` will make it more focused and deterministic. If set to `0`, the model will use log probability to automatically adjust the temperature until certain thresholds are met.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNvm2uSy6Lat",
        "outputId": "e7ac217f-1206-4467-b4e9-e17361698899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The capital of West Bengal is located on the banks of the Huggali River, 180 km from the border of the Bengal Khadi.\n"
          ]
        }
      ],
      "source": [
        "filename = \"./Week_1/Day_5/translation.m4a\"\n",
        "with open(filename, \"rb\") as file:\n",
        "    translation = client.audio.translations.create(\n",
        "      file=(filename, file.read()),\n",
        "      model=\"whisper-large-v3\",\n",
        "      prompt=\"Specify context or spelling\",  # Optional\n",
        "      response_format=\"json\",  # Optional\n",
        "      temperature=0.0  # Optional\n",
        "    )\n",
        "    print(translation.text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWaQYYZ66Sgn"
      },
      "source": [
        "# Vision models\n",
        "\n",
        "Groq API offers fast inference and low latency for multimodal models with vision capabilities for understanding and interpreting visual data from images. By analyzing the content of an image, multimodal models can generate human-readable text for providing insights about given visual data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKk_nQWx6UZv",
        "outputId": "a02d41f0-d903-4bd7-e68b-deed1d6af116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The image shows a two-story room with an industrial design, possibly a cafeteria or office space.\n",
            "\n",
            "**First Floor:**\n",
            "\n",
            "* A long table with chairs is visible on the left side of the room.\n",
            "* There are several windows along the back wall, allowing natural light to enter the room.\n",
            "* A staircase in the center of the room leads to the second floor.\n",
            "* A group of people is standing on the stairs, possibly waiting for something or someone.\n",
            "\n",
            "**Second Floor:**\n",
            "\n",
            "* The second floor has a similar layout to the first floor, with a long table and chairs, as well as additional seating areas.\n",
            "* There are more windows on this floor, providing natural light and a sense of openness.\n",
            "* The walls are made of exposed brick, giving the space a rustic, industrial feel.\n",
            "\n",
            "**Overall:**\n",
            "\n",
            "* The room has a modern and sleek design, with a mix of industrial and natural elements.\n",
            "* The use of exposed brick, metal railings, and wooden accents creates a unique and visually appealing atmosphere.\n"
          ]
        }
      ],
      "source": [
        "# Pass Images from URLs as Input\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"llama-3.2-11b-vision-preview\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"What's in this image?\"\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": \"https://static.wixstatic.com/media/nsplsh_464847566f4f6c77476149~mv2_d_6016_4016_s_4_2.jpg/v1/crop/x_0,y_32,w_6016,h_3951/fill/w_708,h_465,al_c,q_80,usm_0.66_1.00_0.01,enc_avif,quality_auto/Image%20by%20Proxyclick%20Visitor%20Management%20System.jpg\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_tokens=1024,\n",
        "    top_p=1,\n",
        "    stream=False,\n",
        "    stop=None,\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpyrdF_FDSfS",
        "outputId": "94be8866-8bc7-415b-ebaa-d79b956bd814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The image depicts a man sitting in a lotus position, with his legs crossed and hands resting on his knees. He is wearing a white tank top and gray pants, and is seated on a plain background. The overall atmosphere of the image is one of serenity and tranquility, as the man appears to be in a state of deep relaxation and meditation.\n"
          ]
        }
      ],
      "source": [
        "#  Pass Locally Saved Images as Input\n",
        "import base64\n",
        "\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# Path to your image\n",
        "image_path = \"./Week_1/Day_5/yoga.jpeg\"\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                    },\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.2-11b-vision-preview\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
