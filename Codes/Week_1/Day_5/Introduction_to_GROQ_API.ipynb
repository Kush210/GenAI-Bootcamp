{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "1. Get key from [here](https://console.groq.com/keys?_gl=1*1ih2ghe*_gcl_au*MTYyNDMzNzQ4OC4xNzM0OTI3MjM3*_ga*MTYzMDI0MzAxOC4xNzI3MDg1NzIx*_ga_4TD0X2GEZG*MTczNTcxMTUxOS42LjEuMTczNTcxMTY0OS42MC4wLjA.)  and store in `secrets` of `colab` with name `GROQ_API_KEY`\n",
        "2. Install groq: `!pip install groq `\n",
        "\n",
        "---\n",
        "Groq emerged as the first API provider to break the 100 tokens per second generation rate while running Meta’s Llama2-70B parameter model.\n",
        "\n",
        "Groq currently hosts a variety of open-source large language models running on its LPUs (Language Processing Unit) for public access. Access to these demos are available through Groq's website.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ref: https://console.groq.com/docs/text-chat"
      ],
      "metadata": {
        "id": "9QqRIowDeyCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF6xqZN4wh0G",
        "outputId": "3227c0eb-525e-423c-ffee-5aa523e1ef93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.13.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
            "Downloading groq-0.13.1-py3-none-any.whl (109 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/109.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text completion API"
      ],
      "metadata": {
        "id": "YHi-Ldh8ygbn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KkNfSzl9do_9"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "import os\n",
        "client = Groq(\n",
        "    api_key=userdata.get(\"GROQ_API_KEY\"),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters:\n",
        "1. **frequency_penalty**  \n",
        "   - `number or null` (Optional)  \n",
        "   - Defaults to 0  \n",
        "   - Accepts a number between `-2.0 and 2.0`. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same lines.\n",
        "\n",
        "2. **max_tokens**  \n",
        "   - `integer or null` (Optional)  \n",
        "   - The maximum number of tokens that can be generated in the chat completion.  \n",
        "   - The total length of input tokens and generated tokens is limited by the model's context length.\n",
        "\n",
        "3. **messages**  \n",
        "   - `array` (Required)  \n",
        "   - A list of messages comprising the conversation so far.\n",
        "\n",
        "4. **model**  \n",
        "   - `string` (Required)  \n",
        "   - The ID of the model to use.\n",
        "\n",
        "1. **presence_penalty**   `number or null` (Optional)  \n",
        "   - Defaults to 0  \n",
        "   - Number between `-2.0 and 2.0`. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to discuss new topics.\n",
        "\n",
        "2. **response_format**  \n",
        "   - `object or null` (Optional)  \n",
        "   - An object specifying the format in which the model must output.  \n",
        "   - Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees that the message generated by the model is valid JSON.  \n",
        "   - **Important**: When using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message.\n",
        "\n",
        "3. **stream**  \n",
        "   - `boolean or null` (Optional)  \n",
        "   - Defaults to `false`  \n",
        "   - If set to `true`, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.\n",
        "\n",
        "4. **temperature**  \n",
        "   - `number or null` (Optional)  \n",
        "   - Defaults to 1  \n",
        "   - What sampling temperature to use, between 0 and 2. Higher values like `0.8` will make the output more random, while lower values like `0.2` will make it more focused and deterministic. We generally recommend altering this or `top_p`, but not both.\n",
        "\n",
        "5. **tool_choice**  \n",
        "   - `string / object or null` (Optional)  \n",
        "   - Controls which (if any) tool is called by the model.  \n",
        "   - `none` means the model will not call any tool and instead generates a message.  \n",
        "   - `auto` means the model can pick between generating a message or calling one or more tools.  \n",
        "   - `required` means the model must call one or more tools.  \n",
        "   - Specifying a particular tool via `{ \"type\": \"function\", \"function\": { \"name\": \"my_function\" } }` forces the model to call that tool.  \n",
        "   - `none` is the default when no tools are present. `auto` is the default if tools are present.\n",
        "\n",
        "6. **tools**  \n",
        "   - `array or null` (Optional)  \n",
        "   - A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\n",
        "\n",
        "7. **top_logprobs**  \n",
        "   - `integer or null` (Optional)  \n",
        "   - This is not yet supported by any of our models. An integer between `0 and 20` specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.\n",
        "\n",
        "8. **top_p**  \n",
        "   - `number or null` (Optional)  \n",
        "   - Defaults to `1`  \n",
        "   - An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So `0.1` means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature`, but not both.\n",
        "\n",
        "9. **user**  \n",
        "   - `string or null` (Optional)  \n",
        "   - A unique identifier representing your end-user, which can help us monitor and detect abuse.\n"
      ],
      "metadata": {
        "id": "3rn6s6bD7NgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"you are a helpful assistant.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of Yoga\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7EMniuKwQ0x",
        "outputId": "e388871d-32f8-415c-c166-060dbd6b4550"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yoga is a holistic practice that originated in ancient India, combining physical postures, breathing techniques, and meditation to promote overall well-being. The importance of yoga can be understood on multiple levels, including physical, mental, emotional, and spiritual. Here are some key benefits of practicing yoga:\n",
            "\n",
            "**Physical Benefits:**\n",
            "\n",
            "1. **Flexibility and Balance**: Yoga helps increase flexibility, balance, and coordination by stretching and strengthening the muscles.\n",
            "2. **Weight Management**: Yoga can help with weight management by building muscle, improving metabolism, and reducing stress.\n",
            "3. **Improved Posture**: Yoga helps improve posture by strengthening the core and back muscles, reducing the risk of back pain and other musculoskeletal issues.\n",
            "4. **Cardiovascular Health**: Yoga can help lower blood pressure, improve circulation, and reduce the risk of heart disease.\n",
            "\n",
            "**Mental and Emotional Benefits:**\n",
            "\n",
            "1. **Reduced Stress and Anxiety**: Yoga helps reduce stress and anxiety by promoting relaxation, calming the mind, and improving mood.\n",
            "2. **Improved Focus and Concentration**: Yoga improves focus, concentration, and mental clarity by training the mind to stay present and aware.\n",
            "3. **Enhanced Self-Awareness**: Yoga helps develop self-awareness, self-acceptance, and self-compassion, leading to a more positive and confident self-image.\n",
            "4. **Better Sleep**: Yoga can help improve sleep quality, duration, and depth, leading to improved overall health and well-being.\n",
            "\n",
            "**Spiritual Benefits:**\n",
            "\n",
            "1. **Connection to Inner Self**: Yoga helps connect with the inner self, promoting a sense of inner peace, calm, and fulfillment.\n",
            "2. **Spiritual Growth**: Yoga can facilitate spiritual growth, self-realization, and a deeper understanding of the universe and our place in it.\n",
            "3. ** Sense of Community**: Yoga can create a sense of community and connection with others, fostering a supportive and like-minded environment.\n",
            "\n",
            "**Other Benefits:**\n",
            "\n",
            "1. **Improved Immune Function**: Yoga can help boost the immune system, reducing the risk of illness and disease.\n",
            "2. **Increased Energy**: Yoga can increase energy levels, improve vitality, and reduce fatigue.\n",
            "3. **Better Digestion**: Yoga can help improve digestion, reduce symptoms of irritable bowel syndrome (IBS), and promote overall gut health.\n",
            "\n",
            "In summary, yoga is a powerful practice that offers a wide range of benefits, from physical and mental health to spiritual growth and self-awareness. Regular yoga practice can lead to a more balanced, harmonious, and fulfilling life, making it an essential part of a healthy lifestyle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio transcription API\n",
        "Transcribes audio into the input language.\n",
        "\n",
        "Parameters:\n",
        "1. **file**  \n",
        "   - `string` (Required)  \n",
        "   - The audio file object (not the file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n",
        "\n",
        "2. **language**  \n",
        "   - `string` (Optional)  \n",
        "   - The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency.\n",
        "\n",
        "3. **model**  \n",
        "   - `string` (Required)  \n",
        "   - ID of the model to use. Only `whisper-large-v3` is currently available.\n",
        "\n",
        "4. **prompt**  \n",
        "   - `string` (Optional)  \n",
        "   - An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.\n",
        "\n",
        "5. **response_format**  \n",
        "   - `string` (Optional)  \n",
        "   - Defaults to `json`  \n",
        "   - The format of the transcript output, in one of these options: `json`, `text`, or `verbose_json`.\n",
        "\n",
        "6. **temperature**  \n",
        "   - `number` (Optional)  \n",
        "   - Defaults to `0`  \n",
        "   - The sampling temperature, between `0` and `1`. Higher values like `0.8` will make the output more random, while lower values like `0.2` will make it more focused and deterministic. If set to `0`, the model will use log probability to automatically adjust the temperature until certain thresholds are met.\n",
        "\n",
        "7. **timestamp_granularities**  \n",
        "   - `array` (Optional)  \n",
        "   - Defaults to `segment`  \n",
        "   - The timestamp granularities to populate for this transcription. `response_format` must be set to `verbose_json` to use timestamp granularities. Either or both of these options are supported: `word`, or `segment`.  \n",
        "   - Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.\n"
      ],
      "metadata": {
        "id": "jmOZFcB2yyI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"transcription.mp3\"\n",
        "\n",
        "with open(filename, \"rb\") as file:\n",
        "    transcription = client.audio.transcriptions.create(\n",
        "      file=(filename, file.read()),\n",
        "      model=\"whisper-large-v3\",\n",
        "      prompt=\"Specify context or spelling\",\n",
        "      response_format=\"json\",\n",
        "      language=\"en\",\n",
        "      temperature=0.0\n",
        "      )\n",
        "    print(transcription.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPrKOoTdyxo2",
        "outputId": "bfc5cf02-4b26-4ea9-8e60-f34656a8d3d3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The fire that warms us can also consume us. It is not the fault of the fire.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio translation API\n",
        "\n",
        "Translates audio into English.\n",
        "\n",
        "Parameters:\n",
        "1. **file**  \n",
        "   - `string` (Required)  \n",
        "   - The audio file object (not the file name) to translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n",
        "\n",
        "2. **model**  \n",
        "   - `string` (Required)  \n",
        "   - ID of the model to use. Only `whisper-large-v3` is currently available.\n",
        "\n",
        "3. **prompt**  \n",
        "   - `string` (Optional)  \n",
        "   - An optional text to guide the model's style or continue a previous audio segment. The prompt should be in English.\n",
        "\n",
        "4. **response_format**  \n",
        "   - `string` (Optional)  \n",
        "   - Defaults to `json`  \n",
        "   - The format of the translation output, in one of these options: `json`, `text`, or `verbose_json`.\n",
        "\n",
        "5. **temperature**  \n",
        "   - `number` (Optional)  \n",
        "   - Defaults to `0`  \n",
        "   - The sampling temperature, between `0` and `1`. Higher values like `0.8` will make the output more random, while lower values like `0.2` will make it more focused and deterministic. If set to `0`, the model will use log probability to automatically adjust the temperature until certain thresholds are met.\n"
      ],
      "metadata": {
        "id": "Y8QptJIG6HKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"translation.m4a\"\n",
        "with open(filename, \"rb\") as file:\n",
        "    translation = client.audio.translations.create(\n",
        "      file=(filename, file.read()),\n",
        "      model=\"whisper-large-v3\",\n",
        "      prompt=\"Specify context or spelling\",  # Optional\n",
        "      response_format=\"json\",  # Optional\n",
        "      temperature=0.0  # Optional\n",
        "    )\n",
        "    print(translation.text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNvm2uSy6Lat",
        "outputId": "e7ac217f-1206-4467-b4e9-e17361698899"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The capital of West Bengal is located on the banks of the Huggli River, 180 km from the border of the Bengal Khadi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision models\n",
        "\n",
        "Groq API offers fast inference and low latency for multimodal models with vision capabilities for understanding and interpreting visual data from images. By analyzing the content of an image, multimodal models can generate human-readable text for providing insights about given visual data."
      ],
      "metadata": {
        "id": "oWaQYYZ66Sgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass Images from URLs as Input\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"llama-3.2-11b-vision-preview\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"What's in this image?\"\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": \"https://static.wixstatic.com/media/nsplsh_464847566f4f6c77476149~mv2_d_6016_4016_s_4_2.jpg/v1/crop/x_0,y_32,w_6016,h_3951/fill/w_708,h_465,al_c,q_80,usm_0.66_1.00_0.01,enc_avif,quality_auto/Image%20by%20Proxyclick%20Visitor%20Management%20System.jpg\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_tokens=1024,\n",
        "    top_p=1,\n",
        "    stream=False,\n",
        "    stop=None,\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKk_nQWx6UZv",
        "outputId": "a02d41f0-d903-4bd7-e68b-deed1d6af116"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This image shows a modern office space with two levels connected by a staircase. The lower level has a long, narrow room with exposed brick walls and towering windows along its left wall. In this space, there are several long tables topped with rolled-up rugs, around which several empty black chairs can be seen. Six clay amphora vases make up the backsplash, and several computer monitor units and other gear can be seen against the right wall.\n",
            "\n",
            "The staircase includes metal handrails and dark bronze posts supporting a second-story landing that is furnished with the amphora vases. Along the left side of the stairwell as seen in the foreground, a black metal and wood table are pushed against the brick wall. At the middle of the second floor, a group of employees or students in long-sleeved jeans and light blue shirts appear to be, listening to a presentation.\n",
            "\n",
            "The atmosphere of the space is open, bright, and collaborative, conducive to creativity and productivity, making it an effective office environment with a professional and modern aesthetic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Pass Locally Saved Images as Input\n",
        "import base64\n",
        "\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# Path to your image\n",
        "image_path = \"yoga.jpeg\"\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                    },\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.2-11b-vision-preview\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpyrdF_FDSfS",
        "outputId": "94be8866-8bc7-415b-ebaa-d79b956bd814"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This image depicts a man sitting in a lotus position, a yoga pose commonly used for meditation. He is wearing a white tank top and gray pants, with his legs crossed and hands raised in front of him. The background is a solid gray color, suggesting that this may be a promotional image for a yoga class or product.\n"
          ]
        }
      ]
    }
  ]
}