{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq3ePUhc5SS8"
      },
      "source": [
        "# SPEECH TO TEXT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbLmU_q5vJHU"
      },
      "source": [
        "**OPENAI AUDIO TRANSCRIPTIONS:** Transcribes audio into the input language.\n",
        "\n",
        "Args:\n",
        "1. **file:** The audio file object (not file name) to transcribe, in one of these formats:flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n",
        "\n",
        "2. **model:** ID of the model to use. Only whisper-1 (which is powered by our open source Whisper V2 model) is currently available.\n",
        "3. **language:** The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency.\n",
        "\n",
        "4. **prompt:** An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.\n",
        "\n",
        "5. **response_format:** The format of the output, in one of these options: json, text, srt verbose_json, or vtt.\n",
        "\n",
        "6. **temperature:** The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to\n",
        "automatically increase the temperature until certain thresholds are hit.\n",
        "\n",
        "7. **timestamp_granularities:** The timestamp granularities to populate for this transcription.\n",
        "`response_format must be set verbose_json` to use timestamp granularities.\n",
        "\n",
        "Either or both of these options are supported: word, or segment. Note: There\n",
        "is no additional latency for segment timestamps, but generating word timestamps\n",
        "incurs additional latency.\n",
        "\n",
        "8. **extra_headers:** Send extra headers\n",
        "\n",
        "9. **extra_query:** Add additional query parameters to the request\n",
        "\n",
        "10. **extra_body:** Add additional JSON properties to the request\n",
        "\n",
        "11. **timeout:** Override the client-level default timeout for this request, in seconds\n",
        "\n",
        "REF: https://platform.openai.com/docs/guides/speech-to-text\n",
        "\n",
        "\n",
        "NOTE: `add openai api in secrets of colab notebook` and `add audio file in files to convert to text`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c4l64NxRrI7Q"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from IPython.display import Audio, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mRw-9VEpuQFu"
      },
      "outputs": [],
      "source": [
        "# Getting the openai key on google colab\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting up the openai key file on local pc\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables from the .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Retrieve the API key from the environment\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = openai.Client(api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/ashish/Desktop/vettura-genai/Codes\n"
          ]
        }
      ],
      "source": [
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGFOPcfLrv_J",
        "outputId": "87491c5a-f7ea-4031-c4f9-33c815419a29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The fire that warms us can also consume us. It is not the fault of the fire.\n"
          ]
        }
      ],
      "source": [
        "audio_file= open(\"./Week_2/Day_1/transcription.mp3\", \"rb\")\n",
        "transcript = client.audio.transcriptions.create(\n",
        "    model = \"whisper-1\",\n",
        "    file=audio_file\n",
        "    )\n",
        "print(transcript.text) # By default, the response type will be json with the raw text included."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOxt6uiz8pen"
      },
      "source": [
        "**Timestamps**\n",
        "\n",
        "The `timestamp_granularities[]` parameter enables a more structured and timestamped json output format, with timestamps at the segment, word level, or both.\n",
        "\n",
        "*This enables word-level precision for transcripts and video edits, which allows for the removal of specific frames tied to individual words.*\n",
        "\n",
        "`response_format must be set verbose_json` to use timestamp granularities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gbOcrlk5d-5",
        "outputId": "b78ef182-049b-4bea-9bf6-a1451c858f3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TranscriptionWord(end=0.23999999463558197, start=0.0, word='The'), TranscriptionWord(end=0.4000000059604645, start=0.23999999463558197, word='fire'), TranscriptionWord(end=0.699999988079071, start=0.4000000059604645, word='that'), TranscriptionWord(end=0.9399999976158142, start=0.699999988079071, word='warms'), TranscriptionWord(end=1.1799999475479126, start=0.9399999976158142, word='us'), TranscriptionWord(end=1.340000033378601, start=1.1799999475479126, word='can'), TranscriptionWord(end=1.7200000286102295, start=1.340000033378601, word='also'), TranscriptionWord(end=2.0999999046325684, start=1.7200000286102295, word='consume'), TranscriptionWord(end=2.5399999618530273, start=2.0999999046325684, word='us'), TranscriptionWord(end=3.059999942779541, start=3.0199999809265137, word='It'), TranscriptionWord(end=3.2200000286102295, start=3.059999942779541, word='is'), TranscriptionWord(end=3.380000114440918, start=3.2200000286102295, word='not'), TranscriptionWord(end=3.680000066757202, start=3.380000114440918, word='the'), TranscriptionWord(end=3.740000009536743, start=3.680000066757202, word='fault'), TranscriptionWord(end=3.9600000381469727, start=3.740000009536743, word='of'), TranscriptionWord(end=4.059999942779541, start=3.9600000381469727, word='the'), TranscriptionWord(end=4.21999979019165, start=4.059999942779541, word='fire')]\n"
          ]
        }
      ],
      "source": [
        "audio_file = open(\"./Week_2/Day_1/transcription.mp3\", \"rb\")\n",
        "transcript = client.audio.transcriptions.create(\n",
        "file=audio_file,\n",
        "model=\"whisper-1\",\n",
        "response_format=\"verbose_json\",  # response_format must be set verbose_json\n",
        "timestamp_granularities=[\"word\"]\n",
        ")\n",
        "\n",
        "print(transcript.words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHVVeglm5YVC"
      },
      "source": [
        "# TEXT TO SPEECH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDpY5Kh4yFqj"
      },
      "source": [
        "**OPENAI AUDIO API** Generates audio from the input text.\n",
        "The Audio API provides a speech endpoint based on our TTS (text-to-speech) model. It comes with 6 built-in voices and can be used to:\n",
        "\n",
        "* Narrate a written blog post\n",
        "* Produce spoken audio in multiple languages\n",
        "* Give real time audio output using streaming\n",
        "\n",
        "Args:\n",
        "1. **input:** The text to generate audio for. The maximum length is 4096 characters.\n",
        "\n",
        "2. **model:** One of the available TTS models:\n",
        "      * tts-1\n",
        "      * tts-1-hd\n",
        "\n",
        "3. **voice:** The voice to use when generating the audio. Supported voices are\n",
        "      * alloy\n",
        "      * echo\n",
        "      * fable\n",
        "      * onyx\n",
        "      * nova\n",
        "      * shimmer\n",
        "  \n",
        "* Previews of the voices are\n",
        "available in the [Text to Speech Guide](https://platform.openai.com/docs/guides/text-to-speech#voice-options)\n",
        "\n",
        "\n",
        "4. **response_format:** The format to audio in. Supported formats are `mp3(default), opus, aac, flac,\n",
        "      wav, and pcm.`\n",
        "      * Opus: For internet streaming and communication, low latency.\n",
        "      * AAC: For digital audio compression, preferred by YouTube, Android, iOS.\n",
        "      * FLAC: For lossless audio compression, favored by audio enthusiasts for archiving.\n",
        "      * WAV: Uncompressed WAV audio, suitable for low-latency applications to avoid decoding overhead.\n",
        "      * PCM: Similar to WAV but containing the raw samples in 24kHz (16-bit signed, low-endian), without the header.\n",
        "\n",
        "5. **speed:** The speed of the generated audio. Select a value from 0.25 to 4.0. 1.0 is\n",
        "      the default.\n",
        "\n",
        "6. **extra_headers:** Send extra headers\n",
        "\n",
        "7. **extra_query:** Add additional query parameters to the request\n",
        "\n",
        "8. **extra_body:** Add additional JSON properties to the request\n",
        "\n",
        "9. **timeout:** Override the client-level default timeout for this request, in seconds\n",
        "\n",
        "REF: https://platform.openai.com/docs/guides/text-to-speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "VXDGbrQMx8Cc"
      },
      "outputs": [],
      "source": [
        "text = \"Mindfulness is a technique that involves being aware of the present moment without judgment.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyY5pldw_0cO"
      },
      "source": [
        "The `speech` endpoint takes in three key inputs: the `model`, the `text` that should be turned into audio, and the `voice` to be used for the audio generation. A simple request would look like the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "U5OhAjKJxzpJ",
        "outputId": "eb220dde-a6e8-472d-d4ee-c18d94f76c43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/c3/wt1h7fz910v7vxy5x88m2h_r0000gn/T/ipykernel_68903/1601613604.py:7: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
            "  audio.stream_to_file(speech_file_path)\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'Audio' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[259], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m speech_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m audio\u001b[38;5;241m.\u001b[39mstream_to_file(speech_file_path)\n\u001b[0;32m----> 8\u001b[0m display(\u001b[43mAudio\u001b[49m(speech_file_path, autoplay\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Audio' is not defined"
          ]
        }
      ],
      "source": [
        "audio =client.audio.speech.create(\n",
        "    model=\"tts-1\",\n",
        "    voice=\"alloy\",\n",
        "    input=text\n",
        ")\n",
        "speech_file_path = \"audio.mp3\"\n",
        "audio.stream_to_file(speech_file_path)\n",
        "display(Audio(speech_file_path, autoplay=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
