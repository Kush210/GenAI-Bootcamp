{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT API Tutorial for Python\n",
        "\n",
        "This tutorial demonstrates how to interact with OpenAI's ChatGPT models using Python. We will explore step-by-step instructions, including setting up the environment, using API parameters, and handling responses effectively. The tutorial is designed to be comprehensive for beginners and intermediates.\n",
        "\n",
        "## Step 1: Setting Up the Python Environment\n",
        "\n",
        "Before proceeding, ensure you have Python installed. You can download Python from [python.org](https://www.python.org/downloads/).\n",
        "\n",
        "### Install Required Libraries\n",
        "We will use the `openai` library to interact with the ChatGPT API. Install it via pip:\n",
        "\n",
        "```bash\n",
        "pip install openai\n",
        "```\n"
      ],
      "metadata": {
        "id": "_3bmoanXKJeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the libraries\n",
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J5lc6RymF64B",
        "outputId": "7d78f376-45e4-45bd-81f5-9c9f069446b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TOnw3cyAAwf9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Obtaining Your OpenAI API Key\n",
        "\n",
        "To access the API, you need an API key from OpenAI. Follow these steps:\n",
        "\n",
        "1. Sign up or log in to the OpenAI platform: [OpenAI Platform](https://platform.openai.com/).\n",
        "2. Navigate to the **API Keys** section in your account settings.\n",
        "3. Generate a new key and copy it. Store it securely as it is required to authenticate your API requests.\n",
        "\n",
        "Once you have the key, you can set it up in your Python script as follows:\n"
      ],
      "metadata": {
        "id": "ccGQik3dFFkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    api_key=userdata.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
        ")"
      ],
      "metadata": {
        "id": "IRSXcFriD2ir"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Making Your First API Call\n",
        "\n",
        "Here’s a complete example of an API request:\n"
      ],
      "metadata": {
        "id": "Z5yArL-yLrDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me about machine learning.\"}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=150,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lma-fXAGr1N",
        "outputId": "e03769ba-9d64-48a7-bcb5-af6a9cdb3d8e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-AgX1JZbxaY03kuC72PPEeZkwEUEDP', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Machine learning is a subset of artificial intelligence that involves developing algorithms and models that enable computers to learn from and make predictions or decisions based on data. Instead of being explicitly programmed to perform a specific task, a machine learning model is trained on a dataset to recognize patterns and relationships within the data. \\n\\nThere are different types of machine learning approaches, including supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, the model is trained on labeled data, where the input data is paired with the correct output. Unsupervised learning involves training the model on unlabeled data to find patterns or groupings within the data. Reinforcement learning involves training the model through a system of rewards and punishments based on its actions.\\n\\nMachine learning is used in', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1734700069, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=150, prompt_tokens=23, total_tokens=173, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Parameters Documentation\n",
        "\n",
        "## **Messages**\n",
        "- **Type:** `array`  \n",
        "- **Required:** ✅  \n",
        "- **Description:**  \n",
        "  A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, images, and audio.\n",
        "\n",
        "---\n",
        "\n",
        "## **Model**\n",
        "- **Type:** `string`  \n",
        "- **Required:** ✅  \n",
        "- **Description:**  \n",
        "  ID of the model to use. Refer to the model endpoint compatibility table for details on which models work with the Chat API.\n",
        "\n",
        "---\n",
        "\n",
        "## **Store**\n",
        "- **Type:** `boolean | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `false`  \n",
        "- **Description:**  \n",
        "  Whether or not to store the output of this chat completion request for use in model distillation or evaluation products.\n",
        "\n",
        "---\n",
        "\n",
        "## **Reasoning Effort** *(o1 models only)*  \n",
        "- **Type:** `string`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `medium`  \n",
        "- **Description:**  \n",
        "  Constrains effort on reasoning for reasoning models. Supported values: `low`, `medium`, and `high`. Lower effort results in faster responses.\n",
        "\n",
        "---\n",
        "\n",
        "## **Metadata**\n",
        "- **Type:** `object | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Developer-defined tags and values used for filtering completions in the dashboard.\n",
        "\n",
        "---\n",
        "\n",
        "## **Frequency Penalty**\n",
        "- **Type:** `number | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `0`  \n",
        "- **Description:**  \n",
        "  A value between `-2.0` and `2.0`. Positive values penalize new tokens based on their frequency in the text so far, reducing repeated lines.\n",
        "\n",
        "---\n",
        "\n",
        "## **Logit Bias**\n",
        "- **Type:** `map`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `null`  \n",
        "- **Description:**  \n",
        "  Modify the likelihood of specific tokens appearing in the completion. Accepts a JSON object mapping tokens to a bias value between `-100` and `100`.\n",
        "\n",
        "---\n",
        "\n",
        "## **Logprobs**\n",
        "- **Type:** `boolean | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `false`  \n",
        "- **Description:**  \n",
        "  Returns the log probabilities of the output tokens if set to `true`.\n",
        "\n",
        "---\n",
        "\n",
        "## **Top Logprobs**\n",
        "- **Type:** `integer | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Specifies the number of most likely tokens to return at each token position (requires `logprobs` set to `true`).\n",
        "\n",
        "---\n",
        "\n",
        "## **Max Completion Tokens**\n",
        "- **Type:** `integer | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Sets an upper bound on the number of tokens generated for a completion.\n",
        "\n",
        "---\n",
        "\n",
        "## **n**\n",
        "- **Type:** `integer | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `1`  \n",
        "- **Description:**  \n",
        "  Determines how many chat completion choices to generate per input message. For cost efficiency, keep this set to `1`.\n",
        "\n",
        "---\n",
        "\n",
        "## **Modalities**\n",
        "- **Type:** `array | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Specifies the output types (e.g., `[\"text\"]` or `[\"text\", \"audio\"]`).\n",
        "\n",
        "---\n",
        "\n",
        "## **Prediction**\n",
        "- **Type:** `object`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Configuration for predicted outputs, improving response times when parts of the model response are known.\n",
        "\n",
        "---\n",
        "\n",
        "## **Audio**  \n",
        "- **Type:** `object | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Parameters for audio output (required when requesting audio output).\n",
        "\n",
        "---\n",
        "\n",
        "## **Presence Penalty**\n",
        "- **Type:** `number | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `0`  \n",
        "- **Description:**  \n",
        "  A value between `-2.0` and `2.0`. Positive values encourage discussing new topics.\n",
        "\n",
        "---\n",
        "\n",
        "## **Response Format**\n",
        "- **Type:** `object`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Specifies the format the model outputs. Example:  \n",
        "  - `{ \"type\": \"json_schema\", \"json_schema\": {...} }` for structured outputs.  \n",
        "  - `{ \"type\": \"json_object\" }` for JSON mode.\n",
        "\n",
        "---\n",
        "\n",
        "## **Seed**\n",
        "- **Type:** `integer | null`  \n",
        "- **Optional:** ✅ *(Beta)*  \n",
        "- **Description:**  \n",
        "  Enables deterministic sampling for repeated requests with the same parameters.\n",
        "\n",
        "---\n",
        "\n",
        "## **Service Tier**\n",
        "- **Type:** `string | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `auto`  \n",
        "- **Description:**  \n",
        "  Specifies the latency tier for processing requests. Supported values: `auto`, `default`.\n",
        "\n",
        "---\n",
        "\n",
        "## **Stop**\n",
        "- **Type:** `string | array | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `null`  \n",
        "- **Description:**  \n",
        "  Up to 4 sequences where the API stops generating further tokens.\n",
        "\n",
        "---\n",
        "\n",
        "## **Stream**\n",
        "- **Type:** `boolean | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `false`  \n",
        "- **Description:**  \n",
        "  If set, tokens are sent as partial message deltas.\n",
        "\n",
        "---\n",
        "\n",
        "## **Temperature**\n",
        "- **Type:** `number | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `1`  \n",
        "- **Description:**  \n",
        "  Controls output randomness (`0` for deterministic, `2` for random).\n",
        "\n",
        "---\n",
        "\n",
        "## **Top-p**\n",
        "- **Type:** `number | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `1`  \n",
        "- **Description:**  \n",
        "  Enables nucleus sampling, considering tokens within the top-p probability mass.\n",
        "\n",
        "---\n",
        "\n",
        "## **Tools**\n",
        "- **Type:** `array`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  List of tools (functions) the model may call. A maximum of 128 tools is supported.\n",
        "\n",
        "---\n",
        "\n",
        "## **Tool Choice**\n",
        "- **Type:** `string | object`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `none`  \n",
        "- **Description:**  \n",
        "  Controls tool calling behavior (`none`, `auto`, or specifying a specific tool).\n",
        "\n",
        "---\n",
        "\n",
        "## **Parallel Tool Calls**\n",
        "- **Type:** `boolean`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `true`  \n",
        "- **Description:**  \n",
        "  Enables parallel function calls when using tools.\n",
        "\n",
        "---\n",
        "\n",
        "## **User**\n",
        "- **Type:** `string`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  A unique identifier representing the end-user for monitoring abuse.\n",
        "\n",
        "---\n",
        "\n",
        "## **Function Call** *(Deprecated)*  \n",
        "- **Type:** `string | object`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Controls function calls (replaced by `tool_choice`).\n",
        "\n",
        "---\n",
        "\n",
        "## **Functions** *(Deprecated)*  \n",
        "- **Type:** `array`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  List of functions (replaced by `tools`).\n"
      ],
      "metadata": {
        "id": "n5ZCTB3-IpLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UZ9fzgXInel",
        "outputId": "3f5c1334-3024-4050-a78e-74ab48bfa9e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-AgWgRPSmdzJNP77le4xjceRJP8kiD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='This is a test.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1734698775, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=6, prompt_tokens=12, total_tokens=18, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Completion Response Parameters Documentation\n",
        "\n",
        "## **id**\n",
        "- **Type:** `string`  \n",
        "- **Description:**  \n",
        "  A unique identifier for the chat completion response.\n",
        "\n",
        "---\n",
        "\n",
        "## **choices**\n",
        "- **Type:** `array`  \n",
        "- **Description:**  \n",
        "  A list of chat completion choices. There can be multiple choices if `n` (the number of completions) is greater than 1.\n",
        "\n",
        "---\n",
        "\n",
        "## **created**\n",
        "- **Type:** `integer`  \n",
        "- **Description:**  \n",
        "  The Unix timestamp (in seconds) representing when the chat completion was created.\n",
        "\n",
        "---\n",
        "\n",
        "## **model**\n",
        "- **Type:** `string`  \n",
        "- **Description:**  \n",
        "  The model that was used to generate the chat completion.\n",
        "\n",
        "---\n",
        "\n",
        "## **service_tier**\n",
        "- **Type:** `string | null`  \n",
        "- **Description:**  \n",
        "  The service tier used for processing the request. This field is included only if the `service_tier` parameter is specified in the request.\n",
        "\n",
        "---\n",
        "\n",
        "## **system_fingerprint**\n",
        "- **Type:** `string`  \n",
        "- **Description:**  \n",
        "  A fingerprint that represents the backend configuration the model runs with. It can help track changes to the backend configuration that might affect determinism.\n",
        "\n",
        "---\n",
        "\n",
        "## **object**\n",
        "- **Type:** `string`  \n",
        "- **Description:**  \n",
        "  The object type, which will always be `chat.completion`.\n",
        "\n",
        "---\n",
        "\n",
        "## **usage**\n",
        "- **Type:** `object`  \n",
        "- **Description:**  \n",
        "  Usage statistics related to the completion request.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Vd7g932lJnVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 4: Interpreting the Response\n",
        "\n",
        "The API returns a JSON object containing:\n",
        "\n",
        "- **choices**:\n",
        "  - A list of generated responses.\n",
        "  - Access the first response using `response['choices'][0]['message']['content']`.\n",
        "\n",
        "- **usage**:\n",
        "  - Provides token usage details:\n",
        "    - `prompt_tokens`: Tokens used in the input.\n",
        "    - `completion_tokens`: Tokens generated in the response.\n",
        "    - `total_tokens`: Total tokens used.\n",
        "\n",
        "Example output:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"message\": {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Machine learning is a subset of artificial intelligence...\"\n",
        "      }\n",
        "    }\n",
        "  ],\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 23,\n",
        "    \"completion_tokens\": 45,\n",
        "    \"total_tokens\": 68\n",
        "  }\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "id": "NzG3D3KHL67C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Best Practices and Tips\n",
        "\n",
        "### Token Management\n",
        "- Keep track of token usage to avoid exceeding limits.\n",
        "- Use shorter prompts and responses for efficiency.\n",
        "\n",
        "### Parameter Tuning\n",
        "- Experiment with `temperature` and `top_p` for different styles of responses.\n",
        "- Adjust `max_tokens` to control response length.\n",
        "\n",
        "### Logging and Error Handling\n",
        "- Log API responses and token usage for debugging and cost management.\n",
        "- Implement error handling to manage rate limits and API key issues:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vg_DK7EGMIFg"
      }
    }
  ]
}