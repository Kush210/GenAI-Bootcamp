{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT API Tutorial for Python\n",
        "\n",
        "This tutorial demonstrates how to interact with OpenAI's ChatGPT models using Python. We will explore step-by-step instructions, including setting up the environment, using API parameters, and handling responses effectively. The tutorial is designed to be comprehensive for beginners and intermediates.\n",
        "\n",
        "## Step 1: Setting Up the Python Environment\n",
        "\n",
        "Before proceeding, ensure you have Python installed. You can download Python from [python.org](https://www.python.org/downloads/).\n",
        "\n",
        "### Install Required Libraries\n",
        "We will use the `openai` library to interact with the ChatGPT API. Install it via pip:\n",
        "\n",
        "```bash\n",
        "pip install openai\n",
        "```\n"
      ],
      "metadata": {
        "id": "_3bmoanXKJeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the libraries\n",
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J5lc6RymF64B",
        "outputId": "7d78f376-45e4-45bd-81f5-9c9f069446b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TOnw3cyAAwf9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Obtaining Your OpenAI API Key\n",
        "\n",
        "To access the API, you need an API key from OpenAI. Follow these steps:\n",
        "\n",
        "1. Sign up or log in to the OpenAI platform: [OpenAI Platform](https://platform.openai.com/).\n",
        "2. Navigate to the **API Keys** section in your account settings.\n",
        "3. Generate a new key and copy it. Store it securely as it is required to authenticate your API requests.\n",
        "\n",
        "Once you have the key, you can set it up in your Python script as follows:\n"
      ],
      "metadata": {
        "id": "ccGQik3dFFkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    api_key=userdata.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
        ")"
      ],
      "metadata": {
        "id": "IRSXcFriD2ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Making Your First API Call\n",
        "\n",
        "Here’s a complete example of an API request:\n"
      ],
      "metadata": {
        "id": "Z5yArL-yLrDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me about machine learning.\"}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=150,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lma-fXAGr1N",
        "outputId": "e03769ba-9d64-48a7-bcb5-af6a9cdb3d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-AgX1JZbxaY03kuC72PPEeZkwEUEDP', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Machine learning is a subset of artificial intelligence that involves developing algorithms and models that enable computers to learn from and make predictions or decisions based on data. Instead of being explicitly programmed to perform a specific task, a machine learning model is trained on a dataset to recognize patterns and relationships within the data. \\n\\nThere are different types of machine learning approaches, including supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, the model is trained on labeled data, where the input data is paired with the correct output. Unsupervised learning involves training the model on unlabeled data to find patterns or groupings within the data. Reinforcement learning involves training the model through a system of rewards and punishments based on its actions.\\n\\nMachine learning is used in', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1734700069, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=150, prompt_tokens=23, total_tokens=173, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Parameters Documentation\n",
        "\n",
        "## **Messages**\n",
        "- **Type:** `array`  \n",
        "- **Required:** ✅  \n",
        "- **Description:**  \n",
        "  A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, images, and audio.\n",
        "\n",
        "---\n",
        "\n",
        "## **Model**\n",
        "- **Type:** `string`  \n",
        "- **Required:** ✅  \n",
        "- **Description:**  \n",
        "  ID of the model to use. Refer to the model endpoint compatibility table for details on which models work with the Chat API.\n",
        "\n",
        "---\n",
        "\n",
        "## **Store**\n",
        "- **Type:** `boolean | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `false`  \n",
        "- **Description:**  \n",
        "  Whether or not to store the output of this chat completion request for use in model distillation or evaluation products.\n",
        "\n",
        "---\n",
        "\n",
        "## **Reasoning Effort** *(o1 models only)*  \n",
        "- **Type:** `string`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `medium`  \n",
        "- **Description:**  \n",
        "  Constrains effort on reasoning for reasoning models. Supported values: `low`, `medium`, and `high`. Lower effort results in faster responses.\n",
        "\n",
        "---\n",
        "\n",
        "## **Metadata**\n",
        "- **Type:** `object | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Developer-defined tags and values used for filtering completions in the dashboard.\n",
        "\n",
        "---\n",
        "\n",
        "## **Frequency Penalty**\n",
        "- **Type:** `number | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `0`  \n",
        "- **Description:**  \n",
        "  A value between `-2.0` and `2.0`. Positive values penalize new tokens based on their frequency in the text so far, reducing repeated lines.\n",
        "\n",
        "---\n",
        "\n",
        "## **Logit Bias**\n",
        "- **Type:** `map`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `null`  \n",
        "- **Description:**  \n",
        "  Modify the likelihood of specific tokens appearing in the completion. Accepts a JSON object mapping tokens to a bias value between `-100` and `100`.\n",
        "\n",
        "---\n",
        "\n",
        "## **Logprobs**\n",
        "- **Type:** `boolean | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `false`  \n",
        "- **Description:**  \n",
        "  Returns the log probabilities of the output tokens if set to `true`.\n",
        "\n",
        "---\n",
        "\n",
        "## **Top Logprobs**\n",
        "- **Type:** `integer | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Specifies the number of most likely tokens to return at each token position (requires `logprobs` set to `true`).\n",
        "\n",
        "---\n",
        "\n",
        "## **Max Completion Tokens**\n",
        "- **Type:** `integer | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Sets an upper bound on the number of tokens generated for a completion.\n",
        "\n",
        "---\n",
        "\n",
        "## **n**\n",
        "- **Type:** `integer | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `1`  \n",
        "- **Description:**  \n",
        "  Determines how many chat completion choices to generate per input message. For cost efficiency, keep this set to `1`.\n",
        "\n",
        "---\n",
        "\n",
        "## **Modalities**\n",
        "- **Type:** `array | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Specifies the output types (e.g., `[\"text\"]` or `[\"text\", \"audio\"]`).\n",
        "\n",
        "---\n",
        "\n",
        "## **Prediction**\n",
        "- **Type:** `object`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Configuration for predicted outputs, improving response times when parts of the model response are known.\n",
        "\n",
        "---\n",
        "\n",
        "## **Audio**  \n",
        "- **Type:** `object | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Parameters for audio output (required when requesting audio output).\n",
        "\n",
        "---\n",
        "\n",
        "## **Presence Penalty**\n",
        "- **Type:** `number | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `0`  \n",
        "- **Description:**  \n",
        "  A value between `-2.0` and `2.0`. Positive values encourage discussing new topics.\n",
        "\n",
        "---\n",
        "\n",
        "## **Response Format**\n",
        "- **Type:** `object`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Specifies the format the model outputs. Example:  \n",
        "  - `{ \"type\": \"json_schema\", \"json_schema\": {...} }` for structured outputs.  \n",
        "  - `{ \"type\": \"json_object\" }` for JSON mode.\n",
        "\n",
        "---\n",
        "\n",
        "## **Seed**\n",
        "- **Type:** `integer | null`  \n",
        "- **Optional:** ✅ *(Beta)*  \n",
        "- **Description:**  \n",
        "  Enables deterministic sampling for repeated requests with the same parameters.\n",
        "\n",
        "---\n",
        "\n",
        "## **Service Tier**\n",
        "- **Type:** `string | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `auto`  \n",
        "- **Description:**  \n",
        "  Specifies the latency tier for processing requests. Supported values: `auto`, `default`.\n",
        "\n",
        "---\n",
        "\n",
        "## **Stop**\n",
        "- **Type:** `string | array | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `null`  \n",
        "- **Description:**  \n",
        "  Up to 4 sequences where the API stops generating further tokens.\n",
        "\n",
        "---\n",
        "\n",
        "## **Stream**\n",
        "- **Type:** `boolean | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `false`  \n",
        "- **Description:**  \n",
        "  If set, tokens are sent as partial message deltas.\n",
        "\n",
        "---\n",
        "\n",
        "## **Temperature**\n",
        "- **Type:** `number | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `1`  \n",
        "- **Description:**  \n",
        "  Controls output randomness (`0` for deterministic, `2` for random).\n",
        "\n",
        "---\n",
        "\n",
        "## **Top-p**\n",
        "- **Type:** `number | null`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `1`  \n",
        "- **Description:**  \n",
        "  Enables nucleus sampling, considering tokens within the top-p probability mass.\n",
        "\n",
        "---\n",
        "\n",
        "## **Tools**\n",
        "- **Type:** `array`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  List of tools (functions) the model may call. A maximum of 128 tools is supported.\n",
        "\n",
        "---\n",
        "\n",
        "## **Tool Choice**\n",
        "- **Type:** `string | object`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `none`  \n",
        "- **Description:**  \n",
        "  Controls tool calling behavior (`none`, `auto`, or specifying a specific tool).\n",
        "\n",
        "---\n",
        "\n",
        "## **Parallel Tool Calls**\n",
        "- **Type:** `boolean`  \n",
        "- **Optional:** ✅  \n",
        "- **Default:** `true`  \n",
        "- **Description:**  \n",
        "  Enables parallel function calls when using tools.\n",
        "\n",
        "---\n",
        "\n",
        "## **User**\n",
        "- **Type:** `string`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  A unique identifier representing the end-user for monitoring abuse.\n",
        "\n",
        "---\n",
        "\n",
        "## **Function Call** *(Deprecated)*  \n",
        "- **Type:** `string | object`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  Controls function calls (replaced by `tool_choice`).\n",
        "\n",
        "---\n",
        "\n",
        "## **Functions** *(Deprecated)*  \n",
        "- **Type:** `array`  \n",
        "- **Optional:** ✅  \n",
        "- **Description:**  \n",
        "  List of functions (replaced by `tools`).\n"
      ],
      "metadata": {
        "id": "n5ZCTB3-IpLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UZ9fzgXInel",
        "outputId": "3f5c1334-3024-4050-a78e-74ab48bfa9e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-AgWgRPSmdzJNP77le4xjceRJP8kiD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='This is a test.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1734698775, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=6, prompt_tokens=12, total_tokens=18, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Completion Response Parameters Documentation\n",
        "\n",
        "## **id**\n",
        "- **Type:** `string`  \n",
        "- **Description:**  \n",
        "  A unique identifier for the chat completion response.\n",
        "\n",
        "---\n",
        "\n",
        "## **choices**\n",
        "- **Type:** `array`  \n",
        "- **Description:**  \n",
        "  A list of chat completion choices. There can be multiple choices if `n` (the number of completions) is greater than 1.\n",
        "\n",
        "---\n",
        "\n",
        "## **created**\n",
        "- **Type:** `integer`  \n",
        "- **Description:**  \n",
        "  The Unix timestamp (in seconds) representing when the chat completion was created.\n",
        "\n",
        "---\n",
        "\n",
        "## **model**\n",
        "- **Type:** `string`  \n",
        "- **Description:**  \n",
        "  The model that was used to generate the chat completion.\n",
        "\n",
        "---\n",
        "\n",
        "## **service_tier**\n",
        "- **Type:** `string | null`  \n",
        "- **Description:**  \n",
        "  The service tier used for processing the request. This field is included only if the `service_tier` parameter is specified in the request.\n",
        "\n",
        "---\n",
        "\n",
        "## **system_fingerprint**\n",
        "- **Type:** `string`  \n",
        "- **Description:**  \n",
        "  A fingerprint that represents the backend configuration the model runs with. It can help track changes to the backend configuration that might affect determinism.\n",
        "\n",
        "---\n",
        "\n",
        "## **object**\n",
        "- **Type:** `string`  \n",
        "- **Description:**  \n",
        "  The object type, which will always be `chat.completion`.\n",
        "\n",
        "---\n",
        "\n",
        "## **usage**\n",
        "- **Type:** `object`  \n",
        "- **Description:**  \n",
        "  Usage statistics related to the completion request.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Vd7g932lJnVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 4: Interpreting the Response\n",
        "\n",
        "The API returns a JSON object containing:\n",
        "\n",
        "- **choices**:\n",
        "  - A list of generated responses.\n",
        "  - Access the first response using `response['choices'][0]['message']['content']`.\n",
        "\n",
        "- **usage**:\n",
        "  - Provides token usage details:\n",
        "    - `prompt_tokens`: Tokens used in the input.\n",
        "    - `completion_tokens`: Tokens generated in the response.\n",
        "    - `total_tokens`: Total tokens used.\n",
        "\n",
        "Example output:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"message\": {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Machine learning is a subset of artificial intelligence...\"\n",
        "      }\n",
        "    }\n",
        "  ],\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 23,\n",
        "    \"completion_tokens\": 45,\n",
        "    \"total_tokens\": 68\n",
        "  }\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "id": "NzG3D3KHL67C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Best Practices and Tips\n",
        "\n",
        "### Token Management\n",
        "- Keep track of token usage to avoid exceeding limits.\n",
        "- Use shorter prompts and responses for efficiency.\n",
        "\n",
        "### Parameter Tuning\n",
        "- Experiment with `temperature` and `top_p` for different styles of responses.\n",
        "- Adjust `max_tokens` to control response length.\n",
        "\n",
        "### Logging and Error Handling\n",
        "- Log API responses and token usage for debugging and cost management.\n",
        "- Implement error handling to manage rate limits and API key issues:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vg_DK7EGMIFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Using groq for faster inference\n",
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI_uqTQ0nqFU",
        "outputId": "89af3653-16db-4287-e649-093c430175e2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.13.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
            "Downloading groq-0.13.1-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "## create the Groq client\n",
        "client = Groq(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=userdata.get(\"GROQ_API_KEY\"),\n",
        ")\n",
        "\n",
        "# Create a chat completion object\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"you are a helpful assistant.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_9H_1VhnqVn",
        "outputId": "5bf6f807-4996-46e9-e651-8c82eec77296"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models have gained significant attention in recent years due to their widespread applications and implications in various domains. Here are some reasons why fast language models are important:\n",
            "\n",
            "1. **Improved conversational AI**: Fast language models enable developers to build more realistic and conversational AI chatbots, virtual assistants, and voice assistants. This improves the user experience and increases the adoption of these technologies.\n",
            "\n",
            "2. **Enhanced natural language processing (NLP) capabilities**: Fast language models can process and analyze vast amounts of text data quickly and accurately, which is crucial for tasks like sentiment analysis, named entity recognition, and topic modeling.\n",
            "\n",
            "3. **Faster development and deployment**: The speed of language models allows developers to iterate and refine their models quickly, reducing the time and effort required for development and deployment.\n",
            "\n",
            "4. **Real-time applications**: Fast language models can be applied in real-time applications, such as language translation, where instant responses are critical.\n",
            "\n",
            "5. **Improved cybersecurity**: Fast language models can be used to detect and prevent cyber threats more efficiently, as they can quickly scan texts for suspicious patterns and anomalies.\n",
            "\n",
            "6. **Enhanced search and recommendation systems**: Fast language models can improve search and recommendation systems by quickly analyzing user queries and providing personalized results.\n",
            "\n",
            "7. **Better text summarization**: Fast language models can summarize large amounts of text quickly and accurately, reducing the time and effort required for document analysis and review.\n",
            "\n",
            "8. **Improved customer service**: Fast language models can be applied in customer service chatbots to quickly respond to user queries and provide personalized support.\n",
            "\n",
            "9. **Increased efficiency in data entry and documentation**: Fast language models can automate tasks like data entry and documentation, freeing up human resources and improving the accuracy and speed of these tasks.\n",
            "\n",
            "10. **Advancements in cognitive computing and AI research**: The development of fast language models contributes to advancements in cognitive computing and AI research, enabling researchers to explore new areas and applications.\n",
            "\n",
            "In summary, fast language models have the potential to revolutionize various industries and applications by providing accurate, efficient, and real-time natural language processing capabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Object\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"id\": \"34a9110d-c39d-423b-9ab9-9c748747b204\",\n",
        "  \"object\": \"chat.completion\",\n",
        "  \"created\": 1708045122,\n",
        "  \"model\": \"mixtral-8x7b-32768\",\n",
        "  \"system_fingerprint\": \"fp_dbffcd8265\",\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"index\": 0,\n",
        "      \"message\": {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Low latency Large Language Models (LLMs) are important in the field of artificial intelligence and natural language processing (NLP) for several reasons:\\n\\n1. Real-time applications: Low latency LLMs are essential for real-time applications such as chatbots, voice assistants, and real-time translation services. These applications require immediate responses, and high latency can lead to a poor user experience.\\n\\n2. Improved user experience: Low latency LLMs provide a more seamless and responsive user experience. Users are more likely to continue using a service that provides quick and accurate responses, leading to higher user engagement and satisfaction.\\n\\n3. Competitive advantage: In today's fast-paced digital world, businesses that can provide quick and accurate responses to customer inquiries have a competitive advantage. Low latency LLMs can help businesses respond to customer inquiries more quickly, potentially leading to increased sales and customer loyalty.\\n\\n4. Better decision-making: Low latency LLMs can provide real-time insights and recommendations, enabling businesses to make better decisions more quickly. This can be particularly important in industries such as finance, healthcare, and logistics, where quick decision-making can have a significant impact on business outcomes.\\n\\n5. Scalability: Low latency LLMs can handle a higher volume of requests, making them more scalable than high-latency models. This is particularly important for businesses that experience spikes in traffic or have a large user base.\\n\\nIn summary, low latency LLMs are essential for real-time applications, providing a better user experience, enabling quick decision-making, and improving scalability. As the demand for real-time NLP applications continues to grow, the importance of low latency LLMs will only become more critical.\"\n",
        "      },\n",
        "      \"finish_reason\": \"stop\",\n",
        "      \"logprobs\": null\n",
        "    }\n",
        "  ],\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 24,\n",
        "    \"completion_tokens\": 377,\n",
        "    \"total_tokens\": 401,\n",
        "    \"prompt_time\": 0.009,\n",
        "    \"completion_time\": 0.774,\n",
        "    \"total_time\": 0.783\n",
        "  },\n",
        "  \"x_groq\": {\n",
        "    \"id\": \"req_01htzpsmfmew5b4rbmbjy2kv74\"\n",
        "  }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "g_askNDvnupB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BF0y3u3lnrtj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}