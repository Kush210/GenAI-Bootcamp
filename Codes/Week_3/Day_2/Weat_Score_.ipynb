{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en\")\n",
    "model = AutoModel.from_pretrained(\"BAAI/bge-large-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: doctor, Embedding: [-0.12245136  0.09869877 -0.10951994 ... -0.43176612 -0.15773162\n",
      " -0.4168615 ]\n",
      "Word: engineer, Embedding: [-0.42335233  0.2715545  -0.66090804 ... -0.3800297   0.2380309\n",
      " -0.44736806]\n",
      "Word: scientist, Embedding: [-0.05474149  0.21385355 -0.13997476 ... -0.18834884 -0.35558525\n",
      " -0.00399417]\n",
      "Word: nurse, Embedding: [ 0.03429724 -0.04092225 -0.6744635  ... -0.12078612  0.14811628\n",
      " -0.48568416]\n",
      "Word: teacher, Embedding: [ 0.12671818  0.05624894 -0.25352243 ...  0.37910852 -0.5353386\n",
      " -0.2911121 ]\n",
      "Word: receptionist, Embedding: [-0.48184884  0.01627805  0.01125895 ... -0.1010972  -0.4698733\n",
      " -0.3500704 ]\n",
      "Word: man, Embedding: [ 0.21948044  0.23434158 -0.15649559 ... -0.09519699 -0.37215176\n",
      "  0.11097046]\n",
      "Word: male, Embedding: [ 0.3450103   0.0761928  -0.0778226  ... -0.3019996  -0.12306633\n",
      " -0.5901602 ]\n",
      "Word: boy, Embedding: [ 0.11523373  0.25534704 -0.06905752 ... -0.19299932 -0.06677964\n",
      " -0.49130365]\n",
      "Word: woman, Embedding: [-0.28373864  0.18534417 -0.54475147 ... -0.32453522 -0.0022113\n",
      " -0.08404833]\n",
      "Word: female, Embedding: [-0.17517173  0.14650653 -0.4397749  ... -0.29295385  0.15580274\n",
      " -0.55060095]\n",
      "Word: girl, Embedding: [-0.40781012  0.15038629 -0.5439505  ...  0.04688786  0.03627417\n",
      " -0.19152749]\n"
     ]
    }
   ],
   "source": [
    "# List of words\n",
    "words = ['doctor', 'engineer', 'scientist', \n",
    "         'nurse', 'teacher', 'receptionist', \n",
    "         'man', 'male', 'boy', \n",
    "         'woman', 'female', 'girl']\n",
    "\n",
    "# Tokenize and generate embeddings\n",
    "inputs = tokenizer(words, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "embeddings = embeddings.numpy()\n",
    "\n",
    "# Save embeddings in key-value pairs\n",
    "embeddings_dict = {word: embedding for word, embedding in zip(words, embeddings)}\n",
    "\n",
    "# Print the embeddings dictionary\n",
    "for word, embedding in embeddings_dict.items():\n",
    "    print(f\"Word: {word}, Embedding: {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#let's define the sets\n",
    "X = ['doctor', 'engineer', 'scientist']\n",
    "Y = ['nurse', 'teacher', 'receptionist']\n",
    "A = ['man', 'male', 'boy']\n",
    "B = ['woman', 'female', 'girl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Differential Association\n",
    "- The function s computes the differential association of a word w with the sets X and Y.\n",
    "- For each word in X, we compute its cosine similarity with w and then take the mean of these - values to get sim_X.\n",
    "- Similarly, we compute the average cosine similarity between w and each word in Y to get sim_Y.\n",
    "- The function returns the difference between sim_X and sim_Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s(w, X, Y):\n",
    "    sim_X = np.mean([cosine_similarity(embeddings_dict[w].reshape(1, -1), embeddings_dict[x].reshape(1, -1)) for x in X])\n",
    "    sim_Y = np.mean([cosine_similarity(embeddings_dict[w].reshape(1, -1), embeddings_dict[y].reshape(1, -1)) for y in Y])\n",
    "    return sim_X - sim_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020658374"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s('man', X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the WEAT Score\n",
    "- For each word in set A, we compute its differential association with X and Y and sum these values.\n",
    "- Similarly, we compute the sum of differential associations for each word in set B.\n",
    "- The WEAT score is the difference between the two sums.\n",
    "- A positive WEAT score indicates that, on average, words in A are more strongly associated with words in X than words in B are. Conversely, a negative score indicates a stronger association between B and X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEAT score: 0.09056484699249268\n"
     ]
    }
   ],
   "source": [
    "WEAT_score = sum([s(a, X, Y) for a in A]) - sum([s(b, X, Y) for b in B])\n",
    "print(f\"WEAT score: {WEAT_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive WEAT score indicates that the words in set A (male-associated terms) have a stronger association with the occupations in set X (like 'doctor', 'engineer', 'scientist') than they do with occupations in set Y (like 'nurse', 'teacher', 'receptionist'). In contrast, the words in set B (female-associated terms) have a stronger association with occupations in set Y."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
