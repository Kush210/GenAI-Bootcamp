{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Install the necessary libraries\n",
        "%%capture\n",
        "!pip install -q tiktoken openai"
      ],
      "metadata": {
        "id": "zFYuumwOskO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing a Dataset for Fine-Tuning\n",
        "\n",
        "This guide outlines the steps to prepare datasets for **Supervised Fine-Tuning (SFT)** and **Direct Preference Optimization (DPO)**. Both methods require well-structured datasets in JSONL format.\n",
        "\n",
        "---\n",
        "\n",
        "## Supervised Fine-Tuning (SFT)\n",
        "\n",
        "Supervised Fine-Tuning requires a dataset containing demonstration examples of the desired behavior. Each example should consist of a **conversation** formatted like the Chat Completions API.\n",
        "\n",
        "### Dataset Format\n",
        "Each line in the dataset should represent a conversation, where each message contains the following keys:\n",
        "- `role`: The role of the speaker (`system`, `user`, or `assistant`).\n",
        "- `content`: The text of the message.\n",
        "\n",
        "### Example\n",
        "```json\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What's the weather like today?\"}, {\"role\": \"assistant\", \"content\": \"Today is sunny with a high of 75°F.\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Can you tell me a joke?\"}, {\"role\": \"assistant\", \"content\": \"Why don't scientists trust atoms? Because they make up everything!\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"How do I bake a cake?\"}, {\"role\": \"assistant\", \"content\": \"To bake a cake, you'll need flour, sugar, eggs, butter, and baking powder. Mix them, pour the batter into a pan, and bake at 350°F for 30 minutes.\"}]}\n",
        "```\n",
        "\n",
        "# Preparing a Dataset for Direct Preference Optimization (DPO)\n",
        "\n",
        "Direct Preference Optimization (DPO) fine-tuning requires a dataset containing examples of **prompts** paired with a **preferred output** (ideal response) and a **non-preferred output** (suboptimal response). The model learns to prioritize the preferred output during training.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Format\n",
        "\n",
        "Each line in the dataset should be in JSONL format with the following structure:\n",
        "\n",
        "### Keys:\n",
        "1. **`input`**:\n",
        "   - Contains the context or conversation leading to the model’s response.\n",
        "   - Should follow the Chat Completions API format:\n",
        "     - `messages`: A list of messages forming the conversation.\n",
        "       - Each message includes:\n",
        "         - `role`: Role of the speaker (`system`, `user`, `assistant`).\n",
        "         - `content`: Text content of the message.\n",
        "     - Optional fields:\n",
        "       - `tools`: Tools available for the model to use.\n",
        "       - `parallel_tool_calls`: Boolean to indicate if tools can be used concurrently.\n",
        "\n",
        "2. **`preferred_output`**:\n",
        "   - The ideal assistant response for the given input.\n",
        "   - Follows the same message format as the Chat Completions API.\n",
        "\n",
        "3. **`non_preferred_output`**:\n",
        "   - A suboptimal response that demonstrates behavior you want the model to avoid.\n",
        "   - Follows the same message format as the Chat Completions API.\n",
        "\n",
        "---\n",
        "\n",
        "## Example Dataset\n",
        "\n",
        "```jsonl\n",
        "{\n",
        "  \"input\": {\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Hello, can you tell me how cold San Francisco is today?\"\n",
        "      }\n",
        "    ],\n",
        "    \"tools\": [],\n",
        "    \"parallel_tool_calls\": true\n",
        "  },\n",
        "  \"preferred_output\": [\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Today in San Francisco, it is not quite cold as expected. Morning clouds will give way to sunshine, with a high near 68°F (20°C) and a low around 57°F (14°C).\"\n",
        "    }\n",
        "  ],\n",
        "  \"non_preferred_output\": [\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"It is not particularly cold in San Francisco today.\"\n",
        "    }\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "4tJcLoeTC_v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "YzCwBAsjsid3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tiktoken # for token counting\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "XNPo_JLiIv6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from tiktoken import get_encoding\n",
        "\n",
        "def validate_and_estimate_finetuning_data(file_path):\n",
        "    # Setup\n",
        "    format_errors = defaultdict(int)\n",
        "    token_counts = []\n",
        "    total_tokens = 0\n",
        "    encoding = get_encoding(\"cl100k_base\")  # For OpenAI models\n",
        "\n",
        "\n",
        "    # Load the dataset\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        dataset = [json.loads(line) for line in f]\n",
        "\n",
        "    for idx, ex in enumerate(dataset):\n",
        "        if not isinstance(ex, dict):\n",
        "            format_errors[\"data_type\"] += 1\n",
        "            continue\n",
        "\n",
        "        messages = ex.get(\"messages\", None)\n",
        "        if not messages:\n",
        "            format_errors[\"missing_messages_list\"] += 1\n",
        "            continue\n",
        "\n",
        "        # Validate format\n",
        "        conversation_tokens = 0\n",
        "        assistant_message_found = False\n",
        "\n",
        "        for message in messages:\n",
        "            if \"role\" not in message or \"content\" not in message:\n",
        "                format_errors[\"message_missing_key\"] += 1\n",
        "                continue\n",
        "\n",
        "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "                format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
        "                format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "            content = message.get(\"content\", None)\n",
        "            function_call = message.get(\"function_call\", None)\n",
        "\n",
        "            if (not content and not function_call) or not isinstance(content, str):\n",
        "                format_errors[\"missing_content\"] += 1\n",
        "\n",
        "            # Count tokens for each message\n",
        "            try:\n",
        "                message_tokens = len(encoding.encode(message.get(\"content\", \"\")))\n",
        "                conversation_tokens += message_tokens\n",
        "            except Exception as e:\n",
        "                format_errors[\"tokenization_error\"] += 1\n",
        "\n",
        "            if message.get(\"role\") == \"assistant\":\n",
        "                assistant_message_found = True\n",
        "\n",
        "        if not assistant_message_found:\n",
        "            format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "        token_counts.append(conversation_tokens)\n",
        "        total_tokens += conversation_tokens\n",
        "\n",
        "    # Output results\n",
        "    return {\n",
        "        \"format_errors\": dict(format_errors),\n",
        "        \"token_counts\": token_counts,\n",
        "        \"total_tokens\": total_tokens,\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "Afd9kib5IvIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training data\n",
        "file_path = \"/content/train_sample.jsonl\"\n",
        "result = validate_and_estimate_finetuning_data(file_path)\n",
        "\n",
        "# Print Results\n",
        "print(\"Training Data\")\n",
        "print(\"Format Errors:\", result[\"format_errors\"])\n",
        "print(\"Token Counts per Conversation:\", result[\"token_counts\"])\n",
        "print(\"Total Tokens:\", result[\"total_tokens\"])\n",
        "\n",
        "\n",
        "file_path = \"/content/validation_sample.jsonl\"\n",
        "result = validate_and_estimate_finetuning_data(file_path)\n",
        "\n",
        "## Test dataset\n",
        "print(\"\\n\\nTest Data\")\n",
        "print(\"Format Errors:\", result[\"format_errors\"])\n",
        "print(\"Token Counts per Conversation:\", result[\"token_counts\"])\n",
        "print(\"Total Tokens:\", result[\"total_tokens\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYbDNYy8I5m_",
        "outputId": "16285698-97e0-4be7-baf7-74872b8cabd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data\n",
            "Format Errors: {}\n",
            "Token Counts per Conversation: [69, 80, 61, 60, 46, 132, 87, 64, 73, 63, 85, 59, 57, 67, 61, 61, 62, 57, 345, 165, 101, 59, 85, 59, 72, 93, 230, 61, 152, 105, 94, 81, 60, 75, 49, 60, 54, 57, 91, 96, 105, 55, 94, 53, 47, 51, 67, 67, 50, 47, 58, 62, 60, 383, 119, 75, 97, 63, 60, 63, 62, 115, 67, 151, 72, 57, 69, 51, 171, 116, 71, 73, 71, 405, 166, 71, 87, 70, 59, 60, 320, 194, 74, 42, 68, 45, 169, 73, 78, 51, 63, 275, 75, 83, 73, 114, 65, 54, 43, 140]\n",
            "Total Tokens: 9327\n",
            "\n",
            "\n",
            "Test Data\n",
            "Format Errors: {}\n",
            "Token Counts per Conversation: [53, 120, 68, 89, 150, 50, 63, 40, 90, 370, 94, 70, 75, 60, 216, 90, 117, 66, 73, 136, 134, 62, 53, 54, 55, 51, 94, 478, 79, 72, 109, 73, 74, 73, 44, 56, 75, 221, 63, 77, 60, 57, 44, 46, 130, 80, 406, 233, 56, 50, 52, 70, 573, 47, 625, 52, 64, 38, 57, 94]\n",
            "Total Tokens: 6921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## create a client\n",
        "client = OpenAI(api_key = userdata.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "upload the fileobject with the purpose as fine-tuning\n",
        "training = client.files.create(\n",
        "  file=open(\"/content/train_sample.jsonl\", \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")\n",
        "\n",
        "validation = client.files.create(\n",
        "  file=open(\"/content/validation_sample.jsonl\", \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")"
      ],
      "metadata": {
        "id": "uYhU4HpHsl1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## List all the files to choose its id for fine tuning with it's data\n",
        "files = client.files.list()"
      ],
      "metadata": {
        "id": "xbmvRfzMsqpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK7ehqAlzFvX",
        "outputId": "b3a3a1c1-f936-4fe4-9af5-6b5c272008c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[FileObject(id='file-V2EegwinGZqvEUjP9nN3Sr', bytes=40960, created_at=1737951653, filename='validation_sample.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None),\n",
              " FileObject(id='file-QfxYpkRzz5SjrMyb6Hj5zM', bytes=58035, created_at=1737951631, filename='train_sample.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None),\n",
              " FileObject(id='file-5JvUDttZaSirAxLCjZ1Mig', bytes=26955, created_at=1736498794, filename='po_dpo1.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None),\n",
              " FileObject(id='file-9ULPgcXg6o5wcSimoiXjvT', bytes=2328, created_at=1736439185, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None),\n",
              " FileObject(id='file-Mr7DQ4JHjGhoU3dVDAVZgg', bytes=26955, created_at=1736437615, filename='po_dpo1.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None),\n",
              " FileObject(id='file-4MhhJYDixqhMimiwWkqkTa', bytes=2948, created_at=1736239278, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None),\n",
              " FileObject(id='file-CsYjyieUan7NTRKHxromaJ', bytes=6136, created_at=1736238806, filename='test.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Paste the file id into the training_file parameter and choose the model and adjust the hyperparameters if you want to tune it\n",
        "job = client.fine_tuning.jobs.create(\n",
        "    training_file=\"file-QfxYpkRzz5SjrMyb6Hj5zM\",\n",
        "    validation_file=\"file-V2EegwinGZqvEUjP9nN3Sr\",\n",
        "    model = \"gpt-4o-mini-2024-07-18\",\n",
        "    method={\n",
        "        \"type\": \"supervised\",\n",
        "        \"supervised\": {\n",
        "            \"hyperparameters\": {\"n_epochs\": 2},\n",
        "        },\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "K897zJFXzLnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe-nf0RY1OUi",
        "outputId": "97c85f3c-4090-4012-ac21-31e3d56f6f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FineTuningJob(id='ftjob-3EVrblg2OD7p0sN7OeMz7fBO', created_at=1737953249, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=2), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=[], seed=674802057, status='validating_files', trained_tokens=None, training_file='file-QfxYpkRzz5SjrMyb6Hj5zM', validation_file='file-V2EegwinGZqvEUjP9nN3Sr', estimated_finish=None, integrations=[], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=2)), type='supervised'), user_provided_suffix=None)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Listing all the recent jobs\n",
        "all_jobs = client.fine_tuning.jobs.list(limit=10).data\n"
      ],
      "metadata": {
        "id": "s-x_0C2m0UI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Prinint the recent job to get the fine-tuned model name\n",
        "all_jobs[0].finished_at"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLJuhp955NEz",
        "outputId": "46b63ded-444a-4faf-c192-a0f0f61bdc82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1737953877"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import openai\n",
        "\n",
        "# Function to check the status of a fine-tuning job\n",
        "def wait_for_finetuning_completion(job_id, check_interval=45):\n",
        "    while True:\n",
        "        try:\n",
        "            job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
        "\n",
        "            finished_at = job_status.finished_at\n",
        "\n",
        "            if finished_at is not None:\n",
        "                print(\"Fine-tuning job completed.\")\n",
        "                return job_status\n",
        "\n",
        "            print(\"Fine-tuning job not completed. Checking again in 45 seconds...\")\n",
        "            time.sleep(check_interval)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}. Retrying in {check_interval} seconds...\")\n",
        "            time.sleep(check_interval)\n",
        "\n"
      ],
      "metadata": {
        "id": "wnPyrA17DSF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fine_tuning_job_id = \"ftjob-3EVrblg2OD7p0sN7OeMz7fBO\"\n",
        "completed_job = wait_for_finetuning_completion(fine_tuning_job_id)\n",
        "\n",
        "print(\"Fine-tuning job details:\")\n",
        "print(completed_job)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY562gaYFloT",
        "outputId": "461b583b-c264-46ed-dd3f-605741070350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning job not completed. Checking again in 45 seconds...\n",
            "Fine-tuning job not completed. Checking again in 45 seconds...\n",
            "Fine-tuning job not completed. Checking again in 45 seconds...\n",
            "Fine-tuning job not completed. Checking again in 45 seconds...\n",
            "Fine-tuning job not completed. Checking again in 45 seconds...\n",
            "Fine-tuning job not completed. Checking again in 45 seconds...\n",
            "Fine-tuning job not completed. Checking again in 45 seconds...\n",
            "Fine-tuning job not completed. Checking again in 45 seconds...\n",
            "Fine-tuning job not completed. Checking again in 45 seconds...\n",
            "Fine-tuning job not completed. Checking again in 45 seconds...\n",
            "Fine-tuning job not completed. Checking again in 45 seconds...\n",
            "Fine-tuning job not completed. Checking again in 45 seconds...\n",
            "Fine-tuning job not completed. Checking again in 45 seconds...\n",
            "Fine-tuning job completed.\n",
            "Fine-tuning job details:\n",
            "FineTuningJob(id='ftjob-3EVrblg2OD7p0sN7OeMz7fBO', created_at=1737953249, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=1737953877, hyperparameters=Hyperparameters(batch_size=1, learning_rate_multiplier=1.8, n_epochs=2), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=['file-EqDX87Ke6YWH5m9LECJK8p'], seed=674802057, status='running', trained_tokens=None, training_file='file-QfxYpkRzz5SjrMyb6Hj5zM', validation_file='file-V2EegwinGZqvEUjP9nN3Sr', estimated_finish=None, integrations=[], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=1, learning_rate_multiplier=1.8, n_epochs=2)), type='supervised'), user_provided_suffix=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def get_fine_tuning_checkpoints(job_id):\n",
        "    url = f\"https://api.openai.com/v1/fine_tuning/jobs/{job_id}/checkpoints\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {userdata.get('OPENAI_API_KEY')}\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx and 5xx)\n",
        "        # print(response.text)\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "job_id = \"ftjob-3EVrblg2OD7p0sN7OeMz7fBO\"\n",
        "\n",
        "checkpoints = get_fine_tuning_checkpoints(job_id)\n",
        "\n",
        "# Print the result\n",
        "print(checkpoints)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB2NuD2x_uCD",
        "outputId": "4c122978-3b50-48f0-ca71-aa04a14488ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'object': 'list', 'data': [{'object': 'fine_tuning.job.checkpoint', 'id': 'ftckpt_2POvCj2gEPaN2CmL7xwrxPUu', 'created_at': 1737953812, 'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::AuBU7NDH', 'fine_tuning_job_id': 'ftjob-3EVrblg2OD7p0sN7OeMz7fBO', 'metrics': {'step': 200}, 'step_number': 200}, {'object': 'fine_tuning.job.checkpoint', 'id': 'ftckpt_Mc7b6rYTMu7lhrXp0A2cYkpB', 'created_at': 1737953609, 'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::AuBU7zl0:ckpt-step-100', 'fine_tuning_job_id': 'ftjob-3EVrblg2OD7p0sN7OeMz7fBO', 'metrics': {'step': 100}, 'step_number': 100}], 'has_more': False, 'first_id': 'ftckpt_2POvCj2gEPaN2CmL7xwrxPUu', 'last_id': 'ftckpt_Mc7b6rYTMu7lhrXp0A2cYkpB'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " def print_checkpoints(checkpoints):\n",
        "    if \"data\" not in checkpoints or not checkpoints[\"data\"]:\n",
        "        print(\"No checkpoints found.\")\n",
        "        return\n",
        "\n",
        "    print(\"Fine-Tuning Checkpoints:\\n\")\n",
        "    for checkpoint in checkpoints[\"data\"]:\n",
        "        print(f\"Checkpoint ID: {checkpoint['id']}\")\n",
        "        print(f\"  Created At: {checkpoint['created_at']}\")\n",
        "        print(f\"  Fine-Tuned Model Checkpoint: {checkpoint['fine_tuned_model_checkpoint']}\")\n",
        "        print(f\"  Fine-Tuning Job ID: {checkpoint['fine_tuning_job_id']}\")\n",
        "        print(f\"  Metrics:\")\n",
        "        for key, value in checkpoint['metrics'].items():\n",
        "            print(f\"    {key}: {value}\")\n",
        "        print(f\"  Step Number: {checkpoint['step_number']}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    print(f\"Has More: {checkpoints.get('has_more', False)}\")\n",
        "    print(f\"First Checkpoint ID: {checkpoints.get('first_id')}\")\n",
        "    print(f\"Last Checkpoint ID: {checkpoints.get('last_id')}\")\n",
        "\n",
        "print_checkpoints(checkpoints)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQM4bXydGMFK",
        "outputId": "3271e91c-7fb7-43bb-a942-a2d0258a4385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tuning Checkpoints:\n",
            "\n",
            "Checkpoint ID: ftckpt_2POvCj2gEPaN2CmL7xwrxPUu\n",
            "  Created At: 1737953812\n",
            "  Fine-Tuned Model Checkpoint: ft:gpt-4o-mini-2024-07-18:personal::AuBU7NDH\n",
            "  Fine-Tuning Job ID: ftjob-3EVrblg2OD7p0sN7OeMz7fBO\n",
            "  Metrics:\n",
            "    step: 200\n",
            "  Step Number: 200\n",
            "----------------------------------------\n",
            "Checkpoint ID: ftckpt_Mc7b6rYTMu7lhrXp0A2cYkpB\n",
            "  Created At: 1737953609\n",
            "  Fine-Tuned Model Checkpoint: ft:gpt-4o-mini-2024-07-18:personal::AuBU7zl0:ckpt-step-100\n",
            "  Fine-Tuning Job ID: ftjob-3EVrblg2OD7p0sN7OeMz7fBO\n",
            "  Metrics:\n",
            "    step: 100\n",
            "  Step Number: 100\n",
            "----------------------------------------\n",
            "Has More: False\n",
            "First Checkpoint ID: ftckpt_2POvCj2gEPaN2CmL7xwrxPUu\n",
            "Last Checkpoint ID: ftckpt_Mc7b6rYTMu7lhrXp0A2cYkpB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec9CzRDyVNz4",
        "outputId": "02ad920e-7cce-442b-d50d-deb34ddb193d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'object': 'list',\n",
              " 'data': [{'object': 'fine_tuning.job.checkpoint',\n",
              "   'id': 'ftckpt_2POvCj2gEPaN2CmL7xwrxPUu',\n",
              "   'created_at': 1737953812,\n",
              "   'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::AuBU7NDH',\n",
              "   'fine_tuning_job_id': 'ftjob-3EVrblg2OD7p0sN7OeMz7fBO',\n",
              "   'metrics': {'step': 200},\n",
              "   'step_number': 200},\n",
              "  {'object': 'fine_tuning.job.checkpoint',\n",
              "   'id': 'ftckpt_Mc7b6rYTMu7lhrXp0A2cYkpB',\n",
              "   'created_at': 1737953609,\n",
              "   'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::AuBU7zl0:ckpt-step-100',\n",
              "   'fine_tuning_job_id': 'ftjob-3EVrblg2OD7p0sN7OeMz7fBO',\n",
              "   'metrics': {'step': 100},\n",
              "   'step_number': 100}],\n",
              " 'has_more': False,\n",
              " 'first_id': 'ftckpt_2POvCj2gEPaN2CmL7xwrxPUu',\n",
              " 'last_id': 'ftckpt_Mc7b6rYTMu7lhrXp0A2cYkpB'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Inferencing the fine tuned model\n",
        "def query(user_input):\n",
        "  completion = client.chat.completions.create(\n",
        "      model=\"ft:gpt-4o-mini-2024-07-18:personal::AuBU7NDH\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are spirit of Aurobindo answer the user queries in his style.\"},\n",
        "          {\"role\": \"user\", \"content\": user_input }\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "_Yq4WGjdGjMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query(\"Swami can you explain what to do when facing adversities ?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b75wFeJmJnWY",
        "outputId": "78d3e7e5-51b1-4f56-cc16-fb2f5760a762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whenever you meet with an obstacle or a difficulty, don’t get discouraged, don’t lose heart, but draw back into your inner conscious poise, your quietude, your strength, and your certitude of victory; take refuge there and look at the obscurity, the conflict, the difficulty with the eyes of the spirit; and see what it is trying to tell you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cWfAIlWpJ4IP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}