{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zFYuumwOskO6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in ./genai_env/lib/python3.10/site-packages (24.3.1)\n",
            "Collecting pip\n",
            "  Downloading pip-25.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading pip-25.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.3.1\n",
            "    Uninstalling pip-24.3.1:\n",
            "      Successfully uninstalled pip-24.3.1\n",
            "Successfully installed pip-25.0\n"
          ]
        }
      ],
      "source": [
        "## Install the necessary libraries\n",
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy\n",
            "  Using cached numpy-2.2.2-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.2-cp310-cp310-macosx_14_0_arm64.whl (5.4 MB)\n",
            "Installing collected packages: numpy\n",
            "Successfully installed numpy-2.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tiktoken openai\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tJcLoeTC_v9"
      },
      "source": [
        "# Preparing a Dataset for Fine-Tuning\n",
        "\n",
        "This guide outlines the steps to prepare datasets for **Supervised Fine-Tuning (SFT)** and **Direct Preference Optimization (DPO)**. Both methods require well-structured datasets in JSONL format.\n",
        "\n",
        "---\n",
        "\n",
        "## Supervised Fine-Tuning (SFT)\n",
        "\n",
        "Supervised Fine-Tuning requires a dataset containing demonstration examples of the desired behavior. Each example should consist of a **conversation** formatted like the Chat Completions API.\n",
        "\n",
        "### Dataset Format\n",
        "Each line in the dataset should represent a conversation, where each message contains the following keys:\n",
        "- `role`: The role of the speaker (`system`, `user`, or `assistant`).\n",
        "- `content`: The text of the message.\n",
        "\n",
        "### Example\n",
        "```json\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What's the weather like today?\"}, {\"role\": \"assistant\", \"content\": \"Today is sunny with a high of 75°F.\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Can you tell me a joke?\"}, {\"role\": \"assistant\", \"content\": \"Why don't scientists trust atoms? Because they make up everything!\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"How do I bake a cake?\"}, {\"role\": \"assistant\", \"content\": \"To bake a cake, you'll need flour, sugar, eggs, butter, and baking powder. Mix them, pour the batter into a pan, and bake at 350°F for 30 minutes.\"}]}\n",
        "```\n",
        "\n",
        "# Preparing a Dataset for Direct Preference Optimization (DPO)\n",
        "\n",
        "Direct Preference Optimization (DPO) fine-tuning requires a dataset containing examples of **prompts** paired with a **preferred output** (ideal response) and a **non-preferred output** (suboptimal response). The model learns to prioritize the preferred output during training.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Format\n",
        "\n",
        "Each line in the dataset should be in JSONL format with the following structure:\n",
        "\n",
        "### Keys:\n",
        "1. **`input`**:\n",
        "   - Contains the context or conversation leading to the model’s response.\n",
        "   - Should follow the Chat Completions API format:\n",
        "     - `messages`: A list of messages forming the conversation.\n",
        "       - Each message includes:\n",
        "         - `role`: Role of the speaker (`system`, `user`, `assistant`).\n",
        "         - `content`: Text content of the message.\n",
        "     - Optional fields:\n",
        "       - `tools`: Tools available for the model to use.\n",
        "       - `parallel_tool_calls`: Boolean to indicate if tools can be used concurrently.\n",
        "\n",
        "2. **`preferred_output`**:\n",
        "   - The ideal assistant response for the given input.\n",
        "   - Follows the same message format as the Chat Completions API.\n",
        "\n",
        "3. **`non_preferred_output`**:\n",
        "   - A suboptimal response that demonstrates behavior you want the model to avoid.\n",
        "   - Follows the same message format as the Chat Completions API.\n",
        "\n",
        "---\n",
        "\n",
        "## Example Dataset\n",
        "\n",
        "```jsonl\n",
        "{\n",
        "  \"input\": {\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Hello, can you tell me how cold San Francisco is today?\"\n",
        "      }\n",
        "    ],\n",
        "    \"tools\": [],\n",
        "    \"parallel_tool_calls\": true\n",
        "  },\n",
        "  \"preferred_output\": [\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Today in San Francisco, it is not quite cold as expected. Morning clouds will give way to sunshine, with a high near 68°F (20°C) and a low around 57°F (14°C).\"\n",
        "    }\n",
        "  ],\n",
        "  \"non_preferred_output\": [\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"It is not particularly cold in San Francisco today.\"\n",
        "    }\n",
        "  ]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YzCwBAsjsid3"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Only for google colab\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XNPo_JLiIv6y"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tiktoken # for token counting\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Afd9kib5IvIC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from tiktoken import get_encoding\n",
        "\n",
        "def validate_and_estimate_finetuning_data(file_path):\n",
        "    # Setup\n",
        "    format_errors = defaultdict(int)\n",
        "    token_counts = []\n",
        "    total_tokens = 0\n",
        "    encoding = get_encoding(\"cl100k_base\")  # For OpenAI models\n",
        "\n",
        "\n",
        "    # Load the dataset\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        dataset = [json.loads(line) for line in f]\n",
        "\n",
        "    for idx, ex in enumerate(dataset):\n",
        "        if not isinstance(ex, dict):\n",
        "            format_errors[\"data_type\"] += 1\n",
        "            continue\n",
        "\n",
        "        messages = ex.get(\"messages\", None)\n",
        "        if not messages:\n",
        "            format_errors[\"missing_messages_list\"] += 1\n",
        "            continue\n",
        "\n",
        "        # Validate format\n",
        "        conversation_tokens = 0\n",
        "        assistant_message_found = False\n",
        "\n",
        "        for message in messages:\n",
        "            if \"role\" not in message or \"content\" not in message:\n",
        "                format_errors[\"message_missing_key\"] += 1\n",
        "                continue\n",
        "\n",
        "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "                format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
        "                format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "            content = message.get(\"content\", None)\n",
        "            function_call = message.get(\"function_call\", None)\n",
        "\n",
        "            if (not content and not function_call) or not isinstance(content, str):\n",
        "                format_errors[\"missing_content\"] += 1\n",
        "\n",
        "            # Count tokens for each message\n",
        "            try:\n",
        "                message_tokens = len(encoding.encode(message.get(\"content\", \"\")))\n",
        "                conversation_tokens += message_tokens\n",
        "            except Exception as e:\n",
        "                format_errors[\"tokenization_error\"] += 1\n",
        "\n",
        "            if message.get(\"role\") == \"assistant\":\n",
        "                assistant_message_found = True\n",
        "\n",
        "        if not assistant_message_found:\n",
        "            format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "        token_counts.append(conversation_tokens)\n",
        "        total_tokens += conversation_tokens\n",
        "\n",
        "    # Output results\n",
        "    return {\n",
        "        \"format_errors\": dict(format_errors),\n",
        "        \"token_counts\": token_counts,\n",
        "        \"total_tokens\": total_tokens,\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/ashish/Desktop/vettura-genai/Codes\n",
            "/Users/ashish/Desktop/vettura-genai/Codes/Week_3/Day_3/Files/train_data_aurosociety.jsonl\n",
            "/Users/ashish/Desktop/vettura-genai/Codes/Week_3/Day_3/Files/validation_data_aurosociety.jsonl\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "training_File_Path = os.path.join(os.getcwd(),\"Week_3/Day_3/Files/train_data_aurosociety.jsonl\")\n",
        "validation_File_Path = os.path.join(os.getcwd(),\"Week_3/Day_3/Files/validation_data_aurosociety.jsonl\")\n",
        "print(training_File_Path)\n",
        "print(validation_File_Path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYbDNYy8I5m_",
        "outputId": "16285698-97e0-4be7-baf7-74872b8cabd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data\n",
            "Format Errors: {'missing_content': 3}\n",
            "Token Counts per Conversation: [39, 72, 625, 73, 71, 77, 66, 138, 97, 514, 182, 70, 64, 109, 41, 108, 178, 85, 62, 63, 247, 120, 56, 131, 143, 68, 47, 133, 87, 471, 54, 50, 59, 55, 55, 70, 294, 777, 65, 105, 49, 73, 72, 100, 82, 61, 83, 74, 67, 63, 58, 46, 83, 121, 70, 63, 76, 73, 53, 53, 75, 130, 220, 79, 46, 163, 57, 89, 51, 60, 75, 147, 50, 606, 43, 58, 52, 55, 56, 86, 88, 43, 70, 101, 100, 118, 75, 53, 47, 174, 60, 104, 87, 47, 75, 59, 60, 529, 122, 60, 50, 78, 87, 61, 132, 58, 70, 95, 73, 194, 80, 61, 198, 77, 59, 75, 45, 59, 71, 54, 93, 96, 78, 51, 81, 69, 59, 72, 54, 88, 46, 69, 147, 57, 68, 96, 63, 62, 275, 60, 405, 132, 74, 95, 155, 87, 96, 248, 226, 64, 50, 171, 78, 60, 101, 54, 112, 50, 146, 57, 75, 399, 53, 51, 43, 54, 652, 60, 65, 70, 171, 72, 49, 60, 55, 42, 85, 54, 54, 56, 60, 90, 69, 230, 69, 67, 73, 91, 79, 180, 67, 56, 42, 43, 102, 67, 130, 118, 66, 140, 132, 63, 98, 91, 66, 77, 61, 46, 70, 63, 72, 75, 45, 83, 61, 82, 778, 158, 66, 81, 92, 42, 61, 70, 87, 96, 128, 863, 57, 61, 63, 70, 62, 80, 64, 57, 37, 67, 419, 94, 54, 85, 65, 49, 32, 51, 95, 507, 49, 85, 85, 71, 55, 58, 55, 63, 81, 111, 47, 68, 57, 54, 88, 56, 152, 43, 58, 59, 61, 50, 58, 66, 56, 89, 55, 129, 51, 73, 70, 82, 143, 59, 62, 45, 60, 120, 82, 83, 82, 40, 60, 71, 81, 70, 48, 91, 64, 44, 69, 66, 335, 355, 93, 60, 72, 151, 66, 57, 62, 355, 59, 56, 49, 77, 44, 42, 44, 61, 65, 69, 55, 82, 84, 62, 98, 59, 72, 130, 116, 61, 112, 147, 83, 231, 50, 83, 95, 63, 53, 55, 1050, 191, 53, 67, 54, 126, 63, 46, 57, 73, 72, 58, 64, 74, 75, 64, 115, 69, 61, 135, 44, 60, 119, 67, 63, 50, 85, 35, 320, 54, 131, 48, 112, 166, 80, 52, 64, 69, 59, 68, 72, 60, 58, 53, 446, 61, 51, 54, 67, 56, 71, 52, 58, 65, 58, 56, 71, 66, 107, 61, 67, 76, 61, 194, 63, 55, 169, 66, 88, 44, 72, 66, 135, 41, 61, 63, 61, 460, 58, 92, 43, 48, 118, 140, 59, 165, 52, 67, 46, 54, 40, 102, 93, 106, 42, 378, 66, 227, 71, 409, 77, 64, 107, 134, 62, 64, 52, 59, 59, 54, 61, 66, 66, 79, 383, 40, 166, 112, 48, 63, 60, 141, 153, 41, 78, 51, 169, 47, 57, 185, 555, 66, 76, 51, 115, 83, 91, 169, 101, 87, 531, 345, 515, 67, 94, 67, 104, 62, 63, 81, 227, 146, 52, 114, 161, 62, 99, 49, 77, 65, 88, 72, 173, 108, 73, 52, 74, 52, 36, 82, 105, 74, 69, 48, 57, 62, 56, 91, 51]\n",
            "Total Tokens: 53676\n",
            "\n",
            "\n",
            "Test Data\n",
            "Format Errors: {'missing_content': 1}\n",
            "Token Counts per Conversation: [55, 208, 78, 77, 413, 90, 72, 52, 38, 68, 70, 62, 156, 105, 77, 50, 66, 68, 63, 66, 74, 58, 90, 120, 67, 71, 305, 86, 40, 75, 258, 44, 105, 69, 48, 406, 183, 84, 56, 44, 94, 84, 64, 118, 57, 54, 88, 129, 74, 60, 223, 68, 94, 55, 54, 76, 57, 130, 99, 119, 108, 49, 67, 591, 1040, 254, 94, 75, 62, 92, 111, 81, 84, 44, 127, 359, 54, 129, 71, 359, 66, 168, 83, 535, 54, 57, 53, 103, 73, 63, 59, 59, 120, 93, 64, 74, 69, 80, 49, 88, 57, 68, 70, 60, 52, 83, 221, 57, 87, 302, 75, 74, 42, 63, 190, 48, 573, 103, 86, 70, 347, 625, 62, 80, 90, 50, 51, 70, 53, 60, 73, 358, 87, 55, 51, 61, 109, 309, 85, 68, 79, 75, 88, 78, 60, 65, 42, 150, 51, 87, 55, 57, 213, 75, 61, 60, 261, 63, 136, 127, 62, 347, 52, 47, 54, 88, 169, 51, 150, 48, 70, 133, 41, 84, 47, 57, 117, 65, 193, 74, 54, 61, 59, 60, 62, 101, 122, 239, 293, 75, 116, 49, 117, 63, 140, 65, 79, 77, 74, 56, 44, 65, 107, 67, 81, 61, 75, 291, 48, 67, 50, 64, 51, 69, 119, 79, 743, 62, 64, 91, 570, 322, 52, 253, 45, 50, 134, 52, 39, 55, 162, 55, 75, 93, 78, 85, 134, 59, 73, 411, 70, 70, 54, 61, 47, 208, 85, 75, 53, 59, 62, 40, 77, 85, 117, 68, 70, 59, 53, 94, 68, 41, 94, 46, 58, 130, 72, 90, 53, 72, 121, 57, 74, 60, 155, 86, 134, 92, 60, 60, 57, 52, 45, 281, 216, 51, 66, 64, 410, 55, 478, 76, 95, 50, 48, 56, 192, 50, 60, 55, 46, 370, 58, 103, 123, 63, 44, 523, 70, 69, 75, 89, 62, 54, 47, 75, 64, 96, 221, 56, 48, 47, 71, 499, 131, 60, 60, 233, 56, 270, 153, 304, 55, 68, 82, 57, 60, 50, 55, 53, 66, 114, 58, 98, 63, 78, 48]\n",
            "Total Tokens: 38721\n"
          ]
        }
      ],
      "source": [
        "## Training data\n",
        "result = validate_and_estimate_finetuning_data(training_File_Path)\n",
        "\n",
        "# Print Results\n",
        "print(\"Training Data\")\n",
        "print(\"Format Errors:\", result[\"format_errors\"])\n",
        "print(\"Token Counts per Conversation:\", result[\"token_counts\"])\n",
        "print(\"Total Tokens:\", result[\"total_tokens\"])\n",
        "\n",
        "result = validate_and_estimate_finetuning_data(validation_File_Path)\n",
        "\n",
        "## Test dataset\n",
        "print(\"\\n\\nTest Data\")\n",
        "print(\"Format Errors:\", result[\"format_errors\"])\n",
        "print(\"Token Counts per Conversation:\", result[\"token_counts\"])\n",
        "print(\"Total Tokens:\", result[\"total_tokens\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in ./genai_env/lib/python3.10/site-packages (1.0.1)\n",
            "Requirement already satisfied: wandb in ./genai_env/lib/python3.10/site-packages (0.19.5)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in ./genai_env/lib/python3.10/site-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in ./genai_env/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./genai_env/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in ./genai_env/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in ./genai_env/lib/python3.10/site-packages (from wandb) (5.29.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in ./genai_env/lib/python3.10/site-packages (from wandb) (6.1.1)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in ./genai_env/lib/python3.10/site-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in ./genai_env/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in ./genai_env/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in ./genai_env/lib/python3.10/site-packages (from wandb) (2.20.0)\n",
            "Requirement already satisfied: setproctitle in ./genai_env/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in ./genai_env/lib/python3.10/site-packages (from wandb) (75.6.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in ./genai_env/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in ./genai_env/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in ./genai_env/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in ./genai_env/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in ./genai_env/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./genai_env/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./genai_env/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./genai_env/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./genai_env/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in ./genai_env/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "OpenAI API Key loaded successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mashishkumarsahani\u001b[0m (\u001b[33mashishkumarsahani-vettura\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Run for local machine. Do not run on colab\n",
        "!pip install python-dotenv\n",
        "!pip install wandb\n",
        "from dotenv import load_dotenv\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Get the OpenAI API key\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "print(\"OpenAI API Key loaded successfully.\")\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uYhU4HpHsl1G"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleting existing training file: /Users/ashish/Desktop/vettura-genai/Codes/Week_3/Day_3/Files/train_data_aurosociety.jsonl\n",
            "Deleting existing validation file: /Users/ashish/Desktop/vettura-genai/Codes/Week_3/Day_3/Files/validation_data_aurosociety.jsonl\n",
            "Training file uploaded: file-CRNq4Y3RiQNaujDKo8d5cH\n",
            "Validation file uploaded: file-DuGstCdcNcX6gGZMm7LCTh\n"
          ]
        }
      ],
      "source": [
        "## create a client\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# Function to check if a file already exists on OpenAI\n",
        "def get_existing_file_id(filename):\n",
        "    files = client.files.list()\n",
        "    for file in files.data:\n",
        "        if file.filename == filename:\n",
        "            return file.id  # Return the existing file ID\n",
        "    return None  # File does not exist\n",
        "\n",
        "# Function to delete a file by ID\n",
        "def delete_file(file_id):\n",
        "    response = client.files.delete(file_id)\n",
        "    return response.deleted\n",
        "\n",
        "# Check and delete training file\n",
        "file_name = os.path.basename(training_File_Path)\n",
        "training_file_id = get_existing_file_id(file_name)\n",
        "if training_file_id:\n",
        "    print(f\"Deleting existing training file: {training_File_Path}\")\n",
        "    delete_file(training_file_id)\n",
        "\n",
        "# Check and delete validation file\n",
        "file_name = os.path.basename(validation_File_Path)\n",
        "validation_file_id = get_existing_file_id(file_name)\n",
        "if validation_file_id:\n",
        "    print(f\"Deleting existing validation file: {validation_File_Path}\")\n",
        "    delete_file(validation_file_id)\n",
        "\n",
        "# Upload the training file\n",
        "training = client.files.create(\n",
        "    file=open(training_File_Path, \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "print(f\"Training file uploaded: {training.id}\")\n",
        "\n",
        "# Upload the validation file\n",
        "validation = client.files.create(\n",
        "    file=open(validation_File_Path, \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "print(f\"Validation file uploaded: {validation.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xbmvRfzMsqpS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[FileObject(id='file-DuGstCdcNcX6gGZMm7LCTh', bytes=230717, created_at=1738806607, filename='validation_data_aurosociety.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-CRNq4Y3RiQNaujDKo8d5cH', bytes=325644, created_at=1738806605, filename='train_data_aurosociety.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-SjXXLLvHUsdYHGVqS8Ls7C', bytes=3816, created_at=1738806126, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None), FileObject(id='file-V7eSNQnY5i5R923coeCir9', bytes=3958, created_at=1738805149, filename='Sarcastic_Bot_Validation.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-JNz8KtJVgg7nymK5S9NhmK', bytes=6507, created_at=1738805107, filename='Sarcastic_Bot_Training.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-A3JvSRWu9DqGDN3Po4EBsg', bytes=3540, created_at=1738748096, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None), FileObject(id='file-9ouoEMTbSW9SxDBSnJCHgm', bytes=6573, created_at=1738747613, filename='Real_Estate_Sales_Lead_Validation_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-1UoXj3j3Wq4mYPjiEh3QjG', bytes=14867, created_at=1738747015, filename='Real_Estate_Sales_Lead_Data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-RCTSLoh1TDA8pMDta9YdA7', bytes=772, created_at=1738488219, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None), FileObject(id='file-Q3FVHgvcx5eRis4FicqS18', bytes=776, created_at=1738487933, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None), FileObject(id='file-XDmuQQxW6Gyz1Wjh4jocfa', bytes=776, created_at=1738483669, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None), FileObject(id='file-KtbyG2phfc1Cbgru1DA73Z', bytes=968, created_at=1738482202, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None), FileObject(id='file-XcYW5kJHtZhZomjLBtVVHz', bytes=2536, created_at=1738480687, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None), FileObject(id='file-2krEs68v1ohMdZyHZVEYkG', bytes=1756, created_at=1738480023, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None), FileObject(id='file-SozLE8VFWRieo8igGS9EnD', bytes=1752, created_at=1738479541, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None), FileObject(id='file-ByvQk6DdAy5P62mQwRDsfh', bytes=230717, created_at=1738476074, filename='validation_data_aurosociety.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-UHZPFR5KKrsHb5YHdja4pF', bytes=325644, created_at=1738476073, filename='train_data_aurosociety.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-BpgRBULP3iC1PDh8yhV5BV', bytes=230717, created_at=1738476059, filename='validation_data_aurosociety.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-Ax4FTMcA8DFhq2D9YZ4hd1', bytes=325644, created_at=1738476057, filename='train_data_aurosociety.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-EqDX87Ke6YWH5m9LECJK8p', bytes=6568, created_at=1737953881, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None), FileObject(id='file-V2EegwinGZqvEUjP9nN3Sr', bytes=40960, created_at=1737951653, filename='validation_sample.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-QfxYpkRzz5SjrMyb6Hj5zM', bytes=58035, created_at=1737951631, filename='train_sample.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-5JvUDttZaSirAxLCjZ1Mig', bytes=26955, created_at=1736498794, filename='po_dpo1.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-9ULPgcXg6o5wcSimoiXjvT', bytes=2328, created_at=1736439185, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None), FileObject(id='file-Mr7DQ4JHjGhoU3dVDAVZgg', bytes=26955, created_at=1736437615, filename='po_dpo1.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-4MhhJYDixqhMimiwWkqkTa', bytes=2948, created_at=1736239278, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None), FileObject(id='file-CsYjyieUan7NTRKHxromaJ', bytes=6136, created_at=1736238806, filename='test.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)]\n"
          ]
        }
      ],
      "source": [
        "## List all the files to choose its id for fine tuning with it's data\n",
        "files = client.files.list()\n",
        "print(files.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K897zJFXzLnZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FineTuningJob(id='ftjob-B6AaNvTuF5C2EBWluXMCgsKz', created_at=1738807247, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=[], seed=12487095, status='validating_files', trained_tokens=None, training_file='file-CRNq4Y3RiQNaujDKo8d5cH', validation_file='file-DuGstCdcNcX6gGZMm7LCTh', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='aurobindo_bot_finetuning_project', entity=None, name=None, tags=None, run_id='ftjob-B6AaNvTuF5C2EBWluXMCgsKz'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3)), type='supervised'), user_provided_suffix=None)\n"
          ]
        }
      ],
      "source": [
        "## Paste the file id into the training_file parameter and choose the model and adjust the hyperparameters if you want to tune it\n",
        "job = client.fine_tuning.jobs.create(\n",
        "    training_file= training.id,\n",
        "    validation_file=validation.id,\n",
        "    model = \"gpt-4o-mini-2024-07-18\",\n",
        "    method={\n",
        "        \"type\": \"supervised\",\n",
        "        \"supervised\": {\n",
        "            \"hyperparameters\": {\n",
        "                \"n_epochs\": 3,  # Number of epochs\n",
        "                \"batch_size\": 128,  # Batch size\n",
        "                \"learning_rate_multiplier\": 0.8,  # Learning rate scaling factor\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    integrations= [\n",
        "        {\n",
        "            \"type\": \"wandb\",\n",
        "            \"wandb\": {\n",
        "                \"project\": \"aurobindo_bot_finetuning_project\",\n",
        "                \"tags\": [\"bot\", \"aurobindo\", \"finetuning\"]\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "print(job)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "s-x_0C2m0UI0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[FineTuningJob(id='ftjob-B6AaNvTuF5C2EBWluXMCgsKz', created_at=1738807247, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=[], seed=12487095, status='running', trained_tokens=None, training_file='file-CRNq4Y3RiQNaujDKo8d5cH', validation_file='file-DuGstCdcNcX6gGZMm7LCTh', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='aurobindo_bot_finetuning_project', entity=None, name=None, tags=None, run_id='ftjob-B6AaNvTuF5C2EBWluXMCgsKz'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-eMxeJaznbBBFy2yn00LbVV0Q', created_at=1738805439, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal:vettura-model:AxlBzJxd', finished_at=1738806121, hyperparameters=Hyperparameters(batch_size=1, learning_rate_multiplier=1.8, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=['file-SjXXLLvHUsdYHGVqS8Ls7C'], seed=5, status='succeeded', trained_tokens=3828, training_file='file-JNz8KtJVgg7nymK5S9NhmK', validation_file='file-V7eSNQnY5i5R923coeCir9', estimated_finish=None, integrations=[], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=1, learning_rate_multiplier=1.8, n_epochs=3)), type='supervised'), user_provided_suffix='vettura-model'), FineTuningJob(id='ftjob-pGTvfZ7GpZOKPo7iRbStsuxV', created_at=1738747688, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal:vetture-training:AxW62eZ9', finished_at=1738748092, hyperparameters=Hyperparameters(batch_size=1, learning_rate_multiplier=1.8, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=['file-A3JvSRWu9DqGDN3Po4EBsg'], seed=3, status='succeeded', trained_tokens=8106, training_file='file-1UoXj3j3Wq4mYPjiEh3QjG', validation_file='file-9ouoEMTbSW9SxDBSnJCHgm', estimated_finish=None, integrations=[], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=1, learning_rate_multiplier=1.8, n_epochs=3)), type='supervised'), user_provided_suffix='vetture-training-'), FineTuningJob(id='ftjob-QE4mLnrBydYUb3WvQzht80jL', created_at=1738487862, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal::AwQUTwpN', finished_at=1738488215, hyperparameters=Hyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=['file-RCTSLoh1TDA8pMDta9YdA7'], seed=591344194, status='succeeded', trained_tokens=180930, training_file='file-WLCt2reXLKtGS3mwMuc6L2', validation_file='file-D5Gnz57YDjs9AwhrYmAqAY', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='aurobindo_finetuning_project', entity=None, name=None, tags=None, run_id='ftjob-QE4mLnrBydYUb3WvQzht80jL'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-KmYdJDjXxkKxcpqgd45htcb4', created_at=1738487381, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal::AwQPqXdg', finished_at=1738487929, hyperparameters=Hyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=['file-Q3FVHgvcx5eRis4FicqS18'], seed=325306981, status='succeeded', trained_tokens=180930, training_file='file-WLCt2reXLKtGS3mwMuc6L2', validation_file='file-D5Gnz57YDjs9AwhrYmAqAY', estimated_finish=None, integrations=[], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-ZRh5ZrPf18DTN8UFSOBHxKAI', created_at=1738483322, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal::AwPJ4dGw', finished_at=1738483665, hyperparameters=Hyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=['file-XDmuQQxW6Gyz1Wjh4jocfa'], seed=396107210, status='succeeded', trained_tokens=180930, training_file='file-BH2yH5MQDvW9QC1SkixESf', validation_file='file-So9TJjh4r3kGKPiBGBSW5F', estimated_finish=None, integrations=[], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-u2GOigfbzQhdzdkM1fBafKM5', created_at=1738481908, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal::AwOvQurU', finished_at=1738482199, hyperparameters=Hyperparameters(batch_size=32, learning_rate_multiplier=0.5, n_epochs=1), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=['file-KtbyG2phfc1Cbgru1DA73Z'], seed=1923948141, status='succeeded', trained_tokens=60310, training_file='file-BH2yH5MQDvW9QC1SkixESf', validation_file='file-So9TJjh4r3kGKPiBGBSW5F', estimated_finish=None, integrations=[], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=32, learning_rate_multiplier=0.5, n_epochs=1)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-4ZZQy1exT51CzmGTufPXlYA5', created_at=1738480134, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal::AwOWzTHd', finished_at=1738480683, hyperparameters=Hyperparameters(batch_size=32, learning_rate_multiplier=0.5, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=['file-XcYW5kJHtZhZomjLBtVVHz'], seed=1707959225, status='succeeded', trained_tokens=180930, training_file='file-BH2yH5MQDvW9QC1SkixESf', validation_file='file-So9TJjh4r3kGKPiBGBSW5F', estimated_finish=None, integrations=[], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=32, learning_rate_multiplier=0.5, n_epochs=3)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-5mrD99v1qkM8BYIacNBinXOz', created_at=1738479677, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal::AwOMG5ld', finished_at=1738480019, hyperparameters=Hyperparameters(batch_size=32, learning_rate_multiplier=0.5, n_epochs=2), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=['file-2krEs68v1ohMdZyHZVEYkG'], seed=1922073823, status='succeeded', trained_tokens=120620, training_file='file-BH2yH5MQDvW9QC1SkixESf', validation_file='file-So9TJjh4r3kGKPiBGBSW5F', estimated_finish=None, integrations=[], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=32, learning_rate_multiplier=0.5, n_epochs=2)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-HmlOuAAnccMMNXLcbxOIo7yZ', created_at=1738479039, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal::AwOEVHyT', finished_at=1738479537, hyperparameters=Hyperparameters(batch_size=32, learning_rate_multiplier=0.5, n_epochs=2), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=['file-SozLE8VFWRieo8igGS9EnD'], seed=1334935251, status='succeeded', trained_tokens=120620, training_file='file-BH2yH5MQDvW9QC1SkixESf', validation_file='file-So9TJjh4r3kGKPiBGBSW5F', estimated_finish=None, integrations=[], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=32, learning_rate_multiplier=0.5, n_epochs=2)), type='supervised'), user_provided_suffix=None)]\n"
          ]
        }
      ],
      "source": [
        "## Listing all the recent jobs\n",
        "all_jobs = client.fine_tuning.jobs.list(limit=10).data\n",
        "print(all_jobs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLJuhp955NEz",
        "outputId": "46b63ded-444a-4faf-c192-a0f0f61bdc82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FineTuningJob(id='ftjob-B6AaNvTuF5C2EBWluXMCgsKz', created_at=1738807247, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=[], seed=12487095, status='running', trained_tokens=None, training_file='file-CRNq4Y3RiQNaujDKo8d5cH', validation_file='file-DuGstCdcNcX6gGZMm7LCTh', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='aurobindo_bot_finetuning_project', entity=None, name=None, tags=None, run_id='ftjob-B6AaNvTuF5C2EBWluXMCgsKz'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3)), type='supervised'), user_provided_suffix=None)\n",
            "FineTuningJob(id='ftjob-B6AaNvTuF5C2EBWluXMCgsKz', created_at=1738807247, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=[], seed=12487095, status='running', trained_tokens=None, training_file='file-CRNq4Y3RiQNaujDKo8d5cH', validation_file='file-DuGstCdcNcX6gGZMm7LCTh', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='aurobindo_bot_finetuning_project', entity=None, name=None, tags=None, run_id='ftjob-B6AaNvTuF5C2EBWluXMCgsKz'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3)), type='supervised'), user_provided_suffix=None)\n"
          ]
        }
      ],
      "source": [
        "## Prinint the recent job to get the fine-tuned model name\n",
        "print(all_jobs[0])\n",
        "print(client.fine_tuning.jobs.retrieve(all_jobs[0].id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wnPyrA17DSF9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-B6AaNvTuF5C2EBWluXMCgsKz\n",
            "Status: succeeded\n",
            "Fine-tuning job succeeded.\n",
            "Status: FineTuningJob(id='ftjob-B6AaNvTuF5C2EBWluXMCgsKz', created_at=1738807247, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal::AxlcK35f', finished_at=1738807754, hyperparameters=Hyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-zlIVXJ18RvnGhemNfGP9NDlz', result_files=['file-HMGhaRZj585aGiHTvN8BM4'], seed=12487095, status='succeeded', trained_tokens=180930, training_file='file-CRNq4Y3RiQNaujDKo8d5cH', validation_file='file-DuGstCdcNcX6gGZMm7LCTh', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='aurobindo_bot_finetuning_project', entity=None, name=None, tags=None, run_id='ftjob-B6AaNvTuF5C2EBWluXMCgsKz'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=128, learning_rate_multiplier=0.8, n_epochs=3)), type='supervised'), user_provided_suffix=None)\n",
            "Model Name: ft:gpt-4o-mini-2024-07-18:personal::AxlcK35f\n",
            "Result file id: file-HMGhaRZj585aGiHTvN8BM4\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import requests\n",
        "checkpoints = None\n",
        "\n",
        "# Function to get the latest accuracy and loss from checkpoints\n",
        "def get_latest_accuracy(job_id, api_key):\n",
        "    url = f\"https://api.openai.com/v1/fine_tuning/jobs/{job_id}/checkpoints\"\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    checkpoints = response.json().get(\"data\", [])\n",
        "\n",
        "    if not checkpoints:\n",
        "        return None, None  # Return None if no checkpoints are available\n",
        "\n",
        "    # Find the latest checkpoint based on step_number\n",
        "    latest_checkpoint = max(checkpoints, key=lambda c: c[\"step_number\"])\n",
        "    latest_accuracy = latest_checkpoint[\"metrics\"][\"full_valid_mean_token_accuracy\"]\n",
        "    latest_loss = latest_checkpoint[\"metrics\"][\"full_valid_loss\"]\n",
        "    return latest_accuracy, latest_loss\n",
        "\n",
        "# Function to monitor fine-tuning job and print training/validation metrics\n",
        "def monitor_finetuning_progress(job_id, api_key, check_interval=10):\n",
        "    while True:\n",
        "        try:\n",
        "            # Retrieve the fine-tuning job status\n",
        "            job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
        "\n",
        "            # Print basic job details\n",
        "            print(f\"Job ID: {job_status.id}\")\n",
        "            print(f\"Status: {job_status.status}\")\n",
        "\n",
        "            # Check if the job has completed\n",
        "            if job_status.status in [\"succeeded\", \"failed\"]:\n",
        "                print(f\"Fine-tuning job {job_status.status}.\")\n",
        "                model_id = job_status.fine_tuned_model\n",
        "                result_file_id = job_status.result_files[0]\n",
        "                return job_status, model_id, result_file_id\n",
        "            \n",
        "            # Retrieve and print the latest accuracy and loss\n",
        "            latest_accuracy, latest_loss = get_latest_accuracy(job_id, api_key)\n",
        "            if latest_accuracy is not None and latest_loss is not None:\n",
        "                print(f\"Latest Accuracy: {latest_accuracy:.3f}\")\n",
        "                print(f\"Latest Loss: {latest_loss:.3f}\")\n",
        "            else:\n",
        "                print(\"No checkpoints available yet.\")\n",
        "                \n",
        "            # Wait before the next check\n",
        "            print(f\"Checking again in {check_interval} seconds...\\n\")\n",
        "            time.sleep(check_interval)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}. Retrying in {check_interval} seconds...\\n\")\n",
        "            time.sleep(check_interval)\n",
        "\n",
        "\n",
        "# Replace `fine_tuning_job_id` with your actual job ID\n",
        "fine_tuning_job_id = all_jobs[0].id\n",
        "status, model_name, result_file_id = monitor_finetuning_progress(fine_tuning_job_id, api_key, 10)\n",
        "print(f\"Status: {status}\")\n",
        "print(f\"Model Name: {model_name}\")\n",
        "print(f\"Result file id: {result_file_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQM4bXydGMFK",
        "outputId": "3271e91c-7fb7-43bb-a942-a2d0258a4385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'object': 'fine_tuning.job.checkpoint', 'id': 'ftckpt_g9QwdBURXb7hj0LLcUvpHQ1u', 'created_at': 1738807736, 'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::AxlcK35f', 'fine_tuning_job_id': 'ftjob-B6AaNvTuF5C2EBWluXMCgsKz', 'metrics': {'step': 13}, 'step_number': 13}\n",
            "{'object': 'fine_tuning.job.checkpoint', 'id': 'ftckpt_txi1AY2KG9ap3ROd2qfK6hw2', 'created_at': 1738807671, 'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::AxlcJjbK:ckpt-step-10', 'fine_tuning_job_id': 'ftjob-B6AaNvTuF5C2EBWluXMCgsKz', 'metrics': {'step': 10}, 'step_number': 10}\n",
            "{'object': 'fine_tuning.job.checkpoint', 'id': 'ftckpt_JkS6E1LZ0w8OCuya1TahZSmX', 'created_at': 1738807593, 'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::AxlcJyUY:ckpt-step-5', 'fine_tuning_job_id': 'ftjob-B6AaNvTuF5C2EBWluXMCgsKz', 'metrics': {'step': 5}, 'step_number': 5}\n"
          ]
        }
      ],
      "source": [
        "response = requests.get(\n",
        "    f\"https://api.openai.com/v1/fine_tuning/jobs/{all_jobs[0].id}/checkpoints\",\n",
        "    headers={\"Authorization\": f\"Bearer {api_key}\"}\n",
        ")\n",
        "checkpoints = response.json().get(\"data\", [])\n",
        "for checkpoint in checkpoints:\n",
        "    print(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec9CzRDyVNz4",
        "outputId": "02ad920e-7cce-442b-d50d-deb34ddb193d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result File Contents:\n",
            "c3RlcCx0cmFpbl9sb3NzLHRyYWluX2FjY3VyYWN5LHZhbGlkX2xvc3MsdmFsaWRfbWVhbl90b2tlbl9hY2N1cmFjeSx0cmFpbl9tZWFuX3Jld2FyZCxmdWxsX3ZhbGlkYXRpb25fbWVhbl9yZXdhcmQKMSwzLjQ4NzAyLDAuNDExMDIsMy43NDc3NCwwLjM3ODEsLAoyLDMuNjU4MTcsMC4zODc0NiwyLjgxMDE3LDAuNDMyMzQsLAozLDIuODY2MDQsMC40Mjg4NiwyLjU1MzMzLDAuNDQyNTEsLAo0LDIuNjA5MywwLjQzNzQ1LDIuMzgyNDUsMC40NTgzOCwsCjUsMi40NjYyOSwwLjQ1NTM0LDIuMzg5ODIsMC40NTc4NCwsCjYsMi4zNjYyNCwwLjQ2OTAyLDIuMzI5ODcsMC40NzEyLCwKNywyLjI3OTc5LDAuNDc1MDgsMi4zODI0NywwLjQ1ODksLAo4LDIuMjg4MzcsMC40NzQwMSwyLjI2MTA2LDAuNDY5OSwsCjksMi4zMzkzLDAuNDY4OTYsMi4yODc3OCwwLjQ3MDY2LCwKMTAsMi4yMjE4MSwwLjQ3Njg2LDIuMzMwMjksMC40Njg2NywsCjExLDIuMjAyOCwwLjQ5NjU2LDIuMjcwNjYsMC40NzA5NiwsCjEyLDIuMjY3OTEsMC40Nzc2NSwyLjI0MzMyLDAuNDg0MjgsLAoxMywyLjAwMzE5LDAuNTM4NzYsMi4zMDI2LDAuNDY4MDMsLAo=\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def print_result_file_content(file_id, api_key):\n",
        "    # API endpoint to retrieve file content\n",
        "    url = f\"https://api.openai.com/v1/files/{file_id}/content\"\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "    # Request the file content\n",
        "    response = requests.get(url, headers=headers)\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        # Print the contents of the file\n",
        "        print(\"Result File Contents:\")\n",
        "        print(response.text)\n",
        "    else:\n",
        "        print(f\"Failed to retrieve file content. Status Code: {response.status_code}\")\n",
        "        print(f\"Error: {response.json()}\")\n",
        "\n",
        "# Print the result file content\n",
        "print_result_file_content(result_file_id, api_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_Yq4WGjdGjMB"
      },
      "outputs": [],
      "source": [
        "## Inferencing the fine tuned model\n",
        "def query(user_input):\n",
        "  completion = client.chat.completions.create(\n",
        "      model= model_name,\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are spirit of Aurobindo answer the user queries in his style.\"},\n",
        "          {\"role\": \"user\", \"content\": user_input }\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b75wFeJmJnWY",
        "outputId": "78d3e7e5-51b1-4f56-cc16-fb2f5760a762"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The supramental consciousness is the divine life, and as such, it is a life not only free from all ignorance but conscious and conscious of all things as it establishes them. Being self-existence, it is the one existence, and therefore it sustains all existing things, and being eternal, it is to be identified with all things in their eternal truth.\n"
          ]
        }
      ],
      "source": [
        "response = query(\"What is the supermind?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWfAIlWpJ4IP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
