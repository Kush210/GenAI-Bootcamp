{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJshb0UQuHGS",
        "outputId": "f5d7a047-f5de-4d24-e716-af95b68127fa",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lamini in /usr/local/lib/python3.11/dist-packages (3.2.6)\n",
            "Requirement already satisfied: lamini-configuration[yaml] in /usr/local/lib/python3.11/dist-packages (from lamini) (0.8.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from lamini) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from lamini) (4.67.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from lamini) (1.26.4)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.11/dist-packages (from lamini) (4.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from lamini) (2.2.2)\n",
            "Requirement already satisfied: azure-storage-blob in /usr/local/lib/python3.11/dist-packages (from lamini) (12.24.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from lamini) (1.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from lamini) (3.11.11)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (from lamini) (1.9.0.post1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->lamini) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->lamini) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->lamini) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->lamini) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->lamini) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->lamini) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->lamini) (1.18.3)\n",
            "Requirement already satisfied: azure-core>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob->lamini) (1.32.0)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob->lamini) (43.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob->lamini) (4.12.2)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob->lamini) (0.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu->lamini) (24.2)\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from lamini-configuration[yaml]->lamini) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->lamini) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->lamini) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->lamini) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->lamini) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->lamini) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->lamini) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->lamini) (2024.12.14)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->lamini) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->lamini) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->lamini) (3.5.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-storage-blob->lamini) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.1.4->azure-storage-blob->lamini) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob->lamini) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install lamini\n",
        "import lamini\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your API key form https://app.lamini.ai/account\n",
        "lamini.api_key = userdata.get('LAMINI')\n",
        "llm = lamini.Lamini(model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLgMJYm8U7Kt"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('/content/debate_data.jsonl') as f:\n",
        "    responses = [json.loads(line) for line in f]\n",
        "\n",
        "# We want to format the data using Llama 3.1's prompt template: https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/\n",
        "\n",
        "formatted_responses = [\n",
        "  {\n",
        "    \"input\": f'<|begin_of_text|><|start_header_id|>user<|end_header_id|>{r[\"question\"]}<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
        "    \"output\": f'{r[\"answer\"]}',\n",
        "  }\n",
        "  for r in responses\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_responses, test_responses = train_test_split(formatted_responses, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'Training set size: {len(train_responses)}')\n",
        "print(f'Test set size: {len(test_responses)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0mT6Yyy3jd8",
        "outputId": "83e514d6-078e-4d5b-a2d5-fc8cdb2e100a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 57\n",
            "Test set size: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_responses[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmc2xRlhzhWa",
        "outputId": "03cf56b9-dd70-495e-db73-a997214defc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>TAPPER:\\nPresident Biden, you have the mic.<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
              "  'output': 'BIDEN:\\nEvery single thing he said is a lie, every single one. For example, veterans are a hell of a lot better off since I passed the PACT Act. One million of them now have insurance, and their families have it. And their families have it, because what happened, whether was Agent Orange or burn pits, they\\'re all being covered now. And he – his group opposed that.\\nWe\\'re also in a situation where we have great respect for veterans. My – my son spent a year in Iraq, living next to one of those burn pits, came back with stage four glioblastoma. I was recently in – in – in France for D-Day, and I spoke to all – about those heroes that died. I went to the World War II cemetery – World War I cemetery he refused to go to. He was standing with his four-star general, and he told him – he said, \"I don\\'t want to go in there because they\\'re a bunch of losers and suckers.\"\\nMy son was not a loser. He was not a sucker. You\\'re the sucker. You\\'re the loser.'},\n",
              " {'input': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>TAPPER:\\nThank you, President Biden. President Trump, the question was about what would you do to make child care more affordable? If you want to take your minute.<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
              "  'output': \"TRUMP:\\nJust you understand, we have polling. We have other things that do – they rate him the worst because what he's done is so bad. And they rate me – yes, I'll show you. I will show you. And they rate me one of the best. OK.\\nAnd if I'm given another four years, I will be the best. I think I'll be the best. Nobody's ever created an economy like us. Nobody ever cut taxes like us. He's the only one I know. He wants to raise your taxes by four times. He wants to raise everybody's taxes by four times. He wants the Trump tax cuts to expire so everybody, including the two of you are going to pay four to five times. Nobody ever heard of this before.\\nAll my life I'd grow up and I'd see politicians talking about cutting taxes. When we cut taxes, as I said, we did more business. Apple and all these companies, they were bringing money back into our country. The worst president in history by far, and everybody knows it.\"},\n",
              " {'input': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>BASH:\\nThank you.<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
              "  'output': 'BIDEN:\\nAnd, by the way…'},\n",
              " {'input': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>TAPPER:\\nYou have 80 seconds left. My question was, what do you say to those voters who believe that you violated your constitutional oath through your actions, inaction on January 6th, 2021, and worried that you'll do it again?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\",\n",
              "  'output': \"TRUMP:\\nWell, I didn't say that to anybody. I said peacefully and patriotically. And Nancy Pelosi, if you just watched the news from two days ago, on tape to her daughter, who's a documentary filmmaker, they say, what she's saying, oh, no, it's my responsibility, I was responsible for this. Because I offered them 10,000 soldiers or National Guard. And she turned them down. And the mayor of – in writing, by the way, the mayor, in writing turned it down, the mayor of D.C., they turned it down. I offered 10,000 because I could see – I had virtually nothing to do.\\nThey asked me to go make a speech. I could see what was happening. Everybody was saying they're going to be there on January 6th. They're going to be there. And I said, you know what, that's a lot of people coming, you could feel it. You could feel it too. And you could feel it. And I said, they ought to have some National Guard or whatever. And I offered it to her. And she now admits that she turned it down. And it was the same day. She was – I don't know, you can't be very happy with her daughter because it made her into a liar. She said, I take full responsibility for January 6th.\"},\n",
              " {'input': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>TAPPER:\\nGive you a minute, sir.<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
              "  'output': \"TRUMP:\\nI didn't have sex with a porn star, number one.\\nNumber two, that was a case that was started and moved. They moved a high-ranking official, a DOJ, into the Manhattan D.A.'s office to start that case. That case is going to be appealed in one.\\nWe had a very terrible judge, horrible judge, Democrat. The prosecutor were all high-ranking Democrats appointed people and the – both the civil and a criminal.\\nHe basically went after his political opponent because he thought it was going to damage me. But when the public found out about these cases, because they understand it better than he does, he has no idea what these cases are, but when he – they – when they found out about these cases, you know what they did? My poll numbers went up way up. You know that because you're reporting it. And we took in more money in the last two weeks than we've ever taken in the history of any campaign.\\nI don't think any campaign has ever taken hundreds of millions of dollars came pouring in because the public knows it's a scam and it's a guy that's after his political opponent because he can't win fair and square.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.tune(data_or_dataset_id=train_responses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BL-alo9zm6E",
        "outputId": "3dd5deda-227a-4411-a132-cad6d40c66d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data pairs uploaded to local.\n",
            "\n",
            "Your dataset id is: ce2c9330c6f9df1522f6e62b3d6ceee165ff7fef5401bf16974f40543c973015 . Consider using this in the future to train using the same data. \n",
            "Eg: llm.train(data_or_dataset_id='ce2c9330c6f9df1522f6e62b3d6ceee165ff7fef5401bf16974f40543c973015')\n",
            "Tuning job submitted! Check status of job 15874 here: https://api.lamini.ai/train/15874\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'job_id': 15874,\n",
              " 'status': 'CREATED',\n",
              " 'dataset_id': 'ce2c9330c6f9df1522f6e62b3d6ceee165ff7fef5401bf16974f40543c973015'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcpHBpFVXCG-",
        "outputId": "9f0ee791-1837-490c-bcc9-b5ad6138bafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: <|begin_of_text|><|start_header_id|>user<|end_header_id|>TAPPER:\n",
            "President Trump, I want to follow up, if I can, you want –\n",
            "[\n",
            "crosstalk\n",
            "]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "Generated Response: TRUMP:\n",
            "I didn't have sex with a porn star, number one.\n",
            "Number two, that was a case that was started and moved. They moved a high-ranking official, a DOJ, into the Manhattan D.A.'s office to start that case. That case is going to be appealed in one.\n",
            "We had a very terrible judge, horrible judge, Democrat. The prosecutor were all high-ranking Democrats appointed people and the – both the civil and a criminal.\n",
            "He basically went after his political opponent because he thought it was going to damage me. But when the public found out about these cases, because they understand it better than he does, he has no idea what these cases are, but when he – they – when they found out about these cases, you know what they did? My poll numbers went up way up. You know that because you're reporting it. And we took in more money in the last two weeks than we've ever taken in the history of any campaign.\n",
            "I don't think any campaign has ever taken hundreds of millions of dollars came pouring in because the public knows it's a scam and it's a guy that's after his political opponent because he can't win fair and square.\n",
            "Real Answer: TRUMP:\n",
            "Am I allowed to respond to him?\n",
            "\n",
            "Question: <|begin_of_text|><|start_header_id|>user<|end_header_id|>BASH:\n",
            "Let's turn to concerns that voters have about each of you.\n",
            "President Biden, you would be 86 at the end of your second term. How do you address concerns about your capability to handle the toughest job in the world well into your 80s?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "Generated Response: BIDEN:\n",
            "Look, the idea that I'm not capable of doing the job because of my age is ridiculous. I was elected once before. I didn't do a bad job, I was reelected. The idea if I ran again, I'd be elected again. And if I were elected again, I'd be 82 years old, 83, 84, 85, going into office. What are the chances I wouldn't make it through? What are the chances I wouldn't be able to serve? I'd like to see what the doctor said, what the vice president said, when they were asked about my capabilities. They said I was in very good health. I'm in better health than I was 25, 30 years ago. Actually, I'm in better shape than most people I know. And I'd like to see what the – what they said about that.\n",
            "The idea that I'm not capable of doing the job is just ridiculous. It's a bunch of malarkey. And I'll have him out the door.\n",
            "Real Answer: BIDEN:\n",
            "Well, first of all, I spent half my career being – being criticized being the youngest person in politics. I was the second-youngest person ever elected to the United States Senate. And now I'm the oldest. This guy's three years younger and a lot less competent. I think that just look at the record. Look what I've done. Look how I've turned around the horrible situation he left me.\n",
            "As I said, 50 million new jobs, 800,000 manufacturing jobs, more investment in America, over millions – billions of dollars in private investment and – and enterprises that we are growing. We've – by the way, we brought an awful a lot of people – the whole idea of computer chips. We used to have 40 percent of the market. We invented those chips. And we lost it because he was sending people to cheap – to find the cheapest jobs overseas and to bring home a product.\n",
            "So I went – I went to South Korea. I convinced Samsung to invest billions of dollars here in the United States. And then guess what? Those fabs, they call them, to – to build these chips, those fabs pay over $100,000. You don't need a college degree for them. And there's billions, about $40 billion already being invested and being built right now in the United States, creating significant jobs for Americans all over – from all over the world.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tuned_llm = lamini.Lamini(model_name=\"4cd70713007c9db8798ea5a5febcfa2acc85202d74c6cfebdd9dc823439fa8a1\")\n",
        "\n",
        "for response in test_responses[:2]:\n",
        "    q = response[\"input\"]\n",
        "    real_answer = response[\"output\"]\n",
        "    generated_response = tuned_llm.generate(q)\n",
        "    print(f\"Question: {q}\")\n",
        "    print(f\"Generated Response: {generated_response}\")\n",
        "    print(f\"Real Answer: {real_answer}\\n\")\n",
        "\n",
        "\n",
        "# custom question\n",
        "# formatted_prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
        "# response = tuned_llm.generate(formatted_prompt)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}